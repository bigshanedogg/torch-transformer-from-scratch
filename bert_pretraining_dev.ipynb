{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"Hodong_Bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:20:51.097290Z",
     "start_time": "2021-05-25T16:20:49.794812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer.models.bert import Bert\n",
    "from transformer.preprocessors.bert_preprocessor import BertPreprocessor\n",
    "from transformer.preprocessors.blender_bot_preprocessor import RetrieverEncoderPreprocessor\n",
    "from transformer.data.dataset import DatasetInterface, DatasetFromDir\n",
    "from transformer.data.blender_bot_data_loader import RetrieverEncoderDataLoader\n",
    "from transformer.trainers.bert_trainer import BertTrainer\n",
    "from transformer.trainers.blender_bot_trainer import RetrieverEncoderBertTrainer\n",
    "from transformer.trainers.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Picas_Server\n",
    "# dataset_dir = \"/home/picas/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# Korea_Server\n",
    "dataset_dir = \"/home/mnt/guest1\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)\n",
    "\n",
    "# # bigshane_local\n",
    "# dataset_dir = \"D:\\_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-17T08:48:41.057Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./scripts/bert/config/retriever_encoder_pretraining_korea.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported konlpy.tag.Mecab successfully\n",
      "loaded spm_model: '/Users/aibud_dev/_jupyter/spm_model/kor/spoken_pretrain_spm_v30000/'\n"
     ]
    }
   ],
   "source": [
    "spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=config[\"data\"][\"language\"], vocab_size=config[\"model\"][\"vocab_size\"])\n",
    "# spm_model_path = config[\"data\"][\"spm_model_path\"].format(root_dir=config[\"data\"][\"root_dir\"], language=config[\"data\"][\"language\"], vocab_size=config[\"model\"][\"vocab_size\"])\n",
    "preprocessor = RetrieverEncoderPreprocessor(language=config[\"data\"][\"language\"], spm_model_path=spm_model_path, embedding_dict=config[\"model\"][\"embedding_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'temp_dir' has been set to '/Users/aibud_dev/_jupyter/model/temp/20210824_095302/' to save model while training\n",
      "LearningRate schedule has been set to 'transformer_lambda'\n"
     ]
    }
   ],
   "source": [
    "trainer = RetrieverEncoderBertTrainer(temp_dir=dataset_dir+\"/model/temp/\")\n",
    "# trainer = RetrieverEncoderBertTrainer(temp_dir=config[\"train\"][\"temp_save_path\"])\n",
    "trainer.set_lr_update(initial_learning_rate=config[\"optimizer\"][\"initial_learning_rate\"], num_warmup_steps=config[\"train\"][\"num_warmup_steps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = Bert(pad_token_id=preprocessor.spm_tokenizer.special_token_dict[\"pad\"][\"id\"], **config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set criterions & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions, criterion_weights = trainer.get_criterions(pad_token_id=preprocessor.spm_tokenizer.special_token_dict[\"pad\"][\"id\"], **config[\"criterion\"])\n",
    "optimizer = trainer.get_optimizer(model=bert, **config[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model device: cpu\n",
      "Setting criterions device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = BertTrainer.set_device(obj=bert, device=device)\n",
    "optimizer = BertTrainer.set_device(obj=optimizer, device=device)\n",
    "criterions = BertTrainer.set_device(obj=criterions, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_params\n",
    "batch_size = 4\n",
    "nprocs = 1\n",
    "\n",
    "total_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn/\"\n",
    "sample_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn/sample/\"\n",
    "train_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn/train/\"\n",
    "val_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn/val/\"\n",
    "\n",
    "train_dataset = DatasetFromDir(data_dir=sample_data_dir, batch_size=batch_size, device=device, nprocs=nprocs, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"])\n",
    "train_data_loader_params = trainer.get_data_loader_params(dataset=train_dataset, preprocessor=preprocessor, batch_size=batch_size, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "train_data_loader = trainer.create_data_loader(**train_data_loader_params)\n",
    "\n",
    "val_dataset = DatasetFromDir(data_dir=val_data_dir, batch_size=batch_size, device=device, nprocs=nprocs, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"])\n",
    "val_data_loader_params = trainer.get_data_loader_params(dataset=val_dataset, preprocessor=preprocessor, batch_size=batch_size, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "val_data_loader = trainer.create_data_loader(**val_data_loader_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_loader.summary(show_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader encode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_token:\t [0, 105, 119]\n",
      "output_token:\t [0, 105, 119]\n",
      "input_segment:\t [0, 105, 119]\n",
      "input_turn:\t [0, 5, 11, 23, 33, 42, 49, 54, 64, 76, 94, 105]\n"
     ]
    }
   ],
   "source": [
    "# row_idx = 6\n",
    "# inputs, outputs = train_data_loader.get_batch()\n",
    "\n",
    "# print(\"input_token:\\t\", [token_idx for token_idx in range(0, len(inputs[\"token\"][row_idx])) if token_idx==0 or inputs[\"token\"][row_idx][token_idx]==preprocessor.spm_tokenizer.special_token_dict[\"sep\"][\"id\"]])\n",
    "# print(\"output_token:\\t\", [token_idx for token_idx in range(0, len(outputs[\"mlm\"][row_idx])) if token_idx==0 or outputs[\"mlm\"][row_idx][token_idx]==preprocessor.spm_tokenizer.special_token_dict[\"sep\"][\"id\"]])\n",
    "# print(\"input_segment:\\t\", [token_idx for token_idx in range(0, len(inputs[\"segment\"][row_idx])-1) if token_idx==0 or inputs[\"segment\"][row_idx][token_idx]!=inputs[\"segment\"][row_idx][token_idx+1]])\n",
    "# print(\"input_turn:\\t\", [token_idx for token_idx in range(0, len(inputs[\"turn\"][row_idx])-1) if token_idx==0 or inputs[\"turn\"][row_idx][token_idx]!=inputs[\"turn\"][row_idx][token_idx+1]])\n",
    "\n",
    "# for input_token, output_token in zip(preprocessor.decode(inputs[\"token\"]), preprocessor.decode(outputs[\"mlm\"])):\n",
    "#     print(\"input_token:\\t\", input_token)\n",
    "#     print(\"output_token:\\t\", output_token)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "amp = True\n",
    "scaler = None\n",
    "if amp: scaler = torch.cuda.amp.GradScaler()\n",
    "save_per_epoch = -1\n",
    "save_per_batch = -1\n",
    "keep_last = True\n",
    "verbose_per_epoch = 1\n",
    "verbose_per_batch = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.fit(model=bert, train_data_loader=train_data_loader, val_data_loader=None, \n",
    "                      criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                      epoch=epoch, amp=amp, save_per_epoch=save_per_epoch, save_per_batch=save_per_batch, keep_last=keep_last, verbose_per_epoch=verbose_per_epoch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = tqdm(train_data_loader, initial=train_data_loader.iter_start, total=len(train_data_loader))\n",
    "data_iter.iter_size = train_data_loader.iter_end - train_data_loader.iter_start\n",
    "epoch_train_history = trainer.train_epoch(model=bert, data_loader=data_iter, \n",
    "                                          criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                                          amp=amp, scaler=scaler, save_per_batch=save_per_batch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_data_loader):\n",
    "    batch_idx += 1\n",
    "    break\n",
    "    batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "    \n",
    "    loss_dict, acc_dict = trainer.iteration(model=bert, batch=batch,\n",
    "                                            criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                            train=True, amp=amp, scaler=scaler)\n",
    "    \n",
    "    print(loss_dict)\n",
    "    print(acc_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration & data_loader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch = [next(train_data_loader.dataset.__iter__()) for i in range(0, batch_size)]\n",
    "batch_idx = 1\n",
    "batch = train_data_loader.collate_fn(batch=_batch)\n",
    "batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "\n",
    "loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "                                        criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                        train=True, amp=amp, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
