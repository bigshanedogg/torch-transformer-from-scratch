{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T02:55:10.811070Z",
     "start_time": "2021-06-15T02:55:09.470506Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "\n",
    "from transformer.assertions.object_assertion import DataAssertion\n",
    "from transformer.utils.tokenizer import MecabTokenizer, SpmTokenizer\n",
    "from transformer.preprocessor.blender_bot_preprocessor import GeneratorPretrainingPreprocessor\n",
    "from transformer.data.dataset import DatasetInterface, DatasetFromDir\n",
    "from transformer.data.blender_bot_data_loader import GeneratorPretrainingDataLoader\n",
    "from transformer.layers.attention import MultiheadAttention, PositionwiseFeedForward, CodeAttention\n",
    "from transformer.layers.transformer import EncoderLayer, DecoderLayer\n",
    "from transformer.layers.head import LanguageModelingHead, PolyEncoderHead, NextSentencePredictionHead\n",
    "from transformer.layers.utils import get_pad_mask, get_sub_mask\n",
    "from transformer.models.transformer import Encoder, Decoder, Transformer\n",
    "from transformer.trainer.blender_bot_trainer import GeneratorPretrainingTransformerTrainer\n",
    "from transformer.trainer.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T02:55:10.856060Z",
     "start_time": "2021-06-15T02:55:10.812801Z"
    }
   },
   "outputs": [],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Picas_Server\n",
    "# dataset_dir = \"/home/picas/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Korea_Server\n",
    "# dataset_dir = \"/home/guest1\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# bigshane_local\n",
    "dataset_dir = \"D:\\_jupyter\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-17T08:48:41.057Z"
    }
   },
   "outputs": [],
   "source": [
    "# architecture hyperparams\n",
    "# TOBE: # 1 ET-encoder Block, 13 ET-decoder Blocks\n",
    "src_vocab_size = 15000\n",
    "tgt_vocab_size = 15000\n",
    "embedding_dict = {\n",
    "    \"segment\": 2, # context, condition\n",
    "}\n",
    "src_timesteps = 128\n",
    "tgt_timesteps = 128\n",
    "num_heads = 32\n",
    "d_model = 512 # 2560\n",
    "d_ff = 3072 # 4096\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 24 # 24\n",
    "dropout = 0.1\n",
    "\n",
    "# layer details\n",
    "pwff_activation = \"gelu\"\n",
    "linear_activation = \"gelu\"\n",
    "layer_bias = True\n",
    "layer_norm_epsilon = 1e-5\n",
    "layer_initialization = \"normal\"\n",
    "shared_embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_params\n",
    "optimizer_params = {\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.98,\n",
    "    \"optimizer_epsilon\": 1e-5,\n",
    "    \"initial_learning_rate\": 1e-4\n",
    "}\n",
    "num_warmup_steps = 4000\n",
    "\n",
    "# criterion_params\n",
    "criterion_params = {\n",
    "  \"lm\": 1.0,\n",
    "  \"ul\": 0.5\n",
    "}\n",
    "\n",
    "# training_params\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "save_per_epoch = 1\n",
    "save_per_batch = -1\n",
    "keep_last = True\n",
    "verbose_per_epoch = 1\n",
    "verbose_per_batch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_params\n",
    "num_workers = 1 * int(bool(torch.cuda.device_count()))\n",
    "pin_memory = bool(torch.cuda.device_count())\n",
    "encoding = \"UTF-8\"\n",
    "src_sep_tokens = [[\"cls\", \"sep\"], [None, \"sep\"]] # [[\"context\", \"sep\"], [\"candidate\", \"sep\"]]\n",
    "approach = \"ignore\"\n",
    "nprocs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot Import konlpy Mecab tagger: <class 'Exception'> - Install MeCab in order to use it: http://konlpy.org/en/latest/install/\n",
      "Importing MeCab for Windows\n",
      "Imported MeCab for Windows successfully\n",
      "loaded spm_model: 'D:\\_jupyter/spm_model/kor/spoken_pretrain_spm_v15000/'\n"
     ]
    }
   ],
   "source": [
    "src_language = \"kor\"\n",
    "tgt_language = \"kor\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "src_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=src_language, vocab_size=src_vocab_size)\n",
    "tgt_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=tgt_language, vocab_size=tgt_vocab_size)\n",
    "trfr_prep = GeneratorPretrainingPreprocessor(src_language=src_language, tgt_language=tgt_language, src_spm_model_path=src_spm_model_path, tgt_spm_model_path=tgt_spm_model_path, embedding_dict=embedding_dict)\n",
    "src_pad_token_id = trfr_prep.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]\n",
    "tgt_pad_token_id = trfr_prep.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'temp_dir' has been set to './20210818_013419/' to save model while training\n",
      "LearningRate schedule has been set to 'transformer_lambda'\n"
     ]
    }
   ],
   "source": [
    "trainer = GeneratorPretrainingTransformerTrainer()\n",
    "trainer.set_lr_update(initial_learning_rate=optimizer_params[\"initial_learning_rate\"], num_warmup_steps=num_warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = Transformer(src_timesteps=src_timesteps, tgt_timesteps=tgt_timesteps, src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, embedding_dict=embedding_dict, \n",
    "#                     src_pad_token_id=trfr_prep.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], tgt_pad_token_id=trfr_prep.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"],\n",
    "#                     d_model=d_model, d_ff=d_ff, num_heads=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, shared_embedding=shared_embedding,\n",
    "#                     dropout=dropout, pwff_activation=pwff_activation, linear_activation=linear_activation, bias=layer_bias, layer_norm_epsilon=layer_norm_epsilon, initialization=layer_initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./scripts/transformer/config/dialog_pretrain_picas.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    config = json.load(fp)\n",
    "transformer = Transformer(src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, **config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set criterions & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions, criterion_weights = trainer.get_criterions(tgt_timesteps=tgt_timesteps, tgt_vocab_size=tgt_vocab_size, tgt_pad_token_id=trfr_prep.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], lm=criterion_params[\"lm\"], ul=criterion_params[\"ul\"])\n",
    "optimizer = trainer.get_optimizer(model=transformer, initial_learning_rate=optimizer_params[\"initial_learning_rate\"], beta_1=optimizer_params[\"beta_1\"], beta_2=optimizer_params[\"beta_2\"], optimizer_epsilon=optimizer_params[\"optimizer_epsilon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model device: cuda:0\n",
      "Setting criterions device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "transformer = GeneratorPretrainingTransformerTrainer.set_device(obj=transformer, device=device)\n",
    "optimizer = GeneratorPretrainingTransformerTrainer.set_device(obj=optimizer, device=device)\n",
    "criterions = GeneratorPretrainingTransformerTrainer.set_device(obj=criterions, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"KaggleConversation\"\n",
    "# data_dir = dataset_dir + \"/dataset/conversation/{dataset_name}/{language}/multi_turn/\".format(dataset_name=dataset_name, language=config[\"data\"][\"src_language\"])\n",
    "# dataset = DatasetFromDir(data_dir=data_dir, batch_size=batch_size, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"], device=device, nprocs=nprocs)\n",
    "# dialog_data_loader_params = TransformerDialogPreTrainer.get_data_loader_params(dataset=dataset, preprocessor=trfr_prep, batch_size=batch_size, \n",
    "#                                                                                device=device, nprocs=nprocs, num_workers=dialog_pretrainer.num_workers, pin_memory=dialog_pretrainer.pin_memory,\n",
    "#                                                                                **config[\"model\"], **config[\"data_loader\"])\n",
    "# kaggle_conversation_data_loader = dialog_pretrainer.create_data_loader(**dialog_data_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/{language}/multi_turn/\".format(language=config[\"data\"][\"src_language\"])\n",
    "# data_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/{language}/multi_turn/sample/\".format(language=config[\"data\"][\"src_language\"])\n",
    "dataset_name = \"KaggleConversation\"\n",
    "data_dir = dataset_dir + \"/dataset/conversation/{dataset_name}/{language}/multi_turn\".format(language=config[\"data\"][\"src_language\"], dataset_name=dataset_name)\n",
    "\n",
    "dataset = DatasetFromDir(data_dir=data_dir, batch_size=batch_size, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"], device=device, nprocs=nprocs)\n",
    "\n",
    "data_loader_params = GeneratorPretrainingTransformerTrainer.get_data_loader_params(dataset=dataset, preprocessor=trfr_prep, batch_size=batch_size, \n",
    "                                                                               device=device, nprocs=nprocs, num_workers=trainer.num_workers, pin_memory=trainer.pin_memory,\n",
    "                                                                               **config[\"model\"], **config[\"data_loader\"])\n",
    "train_data_loader = trainer.create_data_loader(**data_loader_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "amp = True\n",
    "scaler = None\n",
    "if amp: scaler = torch.cuda.amp.GradScaler()\n",
    "save_per_epoch = -1\n",
    "save_per_batch = -1\n",
    "keep_last = True\n",
    "verbose_per_epoch = 1\n",
    "verbose_per_batch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dialog_pretrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e7a9e257d813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = dialog_pretrainer.fit(model=transformer, train_data_loader=dialog_pretrain_data_loader, val_data_loader=None, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                 \u001b[0mcriterions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                 epoch=epoch, amp=amp, save_per_epoch=save_per_epoch, save_per_batch=save_per_batch, keep_last=keep_last, verbose_per_epoch=verbose_per_epoch, verbose_per_batch=verbose_per_batch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dialog_pretrainer' is not defined"
     ]
    }
   ],
   "source": [
    "history = dialog_pretrainer.fit(model=transformer, train_data_loader=dialog_pretrain_data_loader, val_data_loader=None, \n",
    "                                criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                                epoch=epoch, amp=amp, save_per_epoch=save_per_epoch, save_per_batch=save_per_batch, keep_last=keep_last, verbose_per_epoch=verbose_per_epoch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pretrain_data_iter = tqdm(dialog_pretrain_data_loader, initial=dialog_pretrain_data_loader.iter_start, total=len(dialog_pretrain_data_loader))\n",
    "dialog_pretrain_data_iter.iter_size = dialog_pretrain_data_loader.iter_end - dialog_pretrain_data_loader.iter_start\n",
    "epoch_train_history = dialog_pretrainer.train_epoch(model=transformer, data_loader=dialog_pretrain_data_iter, \n",
    "                                                    criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                                                    amp=amp, scaler=scaler, save_per_batch=save_per_batch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(dialog_pretrain_data_loader):\n",
    "    batch_idx += 1\n",
    "    batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "\n",
    "    loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "                                            criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                            train=True, amp=amp, scaler=scaler)\n",
    "\n",
    "    print(loss_dict)\n",
    "    print(acc_dict)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting token_ids: 100%|██████████████████████████████████████████████████| 304082/304082 [00:45<00:00, 6691.13it/s]\n",
      "Normalizing distribution: 100%|█████████████████████████████████████████████████| 5964/5964 [00:00<00:00, 17706.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted prev_token_distribution from total 5964 tokens\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"D:\\_jupyter\\dataset\\conversation\\SelectStar\\kor\\multi_turn/\"\n",
    "dataset = DatasetFromDir(data_dir=data_dir, batch_size=4, encoding=\"utf-8\", extension=\"json\", device=\"cuda:0\", nprocs=1)\n",
    "data = dataset.get_all_data()\n",
    "\n",
    "utterances = [utterance for row in data for utterance in row[\"utterances\"]]\n",
    "target_prev_token_distribution = trfr_prep.extract_prev_token_distribution(sentences=utterances, ngram=5)\n",
    "\n",
    "from collections import Counter\n",
    "predicted_prev_token_distribution = dict()\n",
    "for k, v in target_prev_token_distribution.items():\n",
    "    counter = Counter()\n",
    "    for _k,_v in v.items():\n",
    "        counter[_k] = np.random.randint(0, 10000)\n",
    "    predicted_prev_token_distribution[k] = counter\n",
    "    \n",
    "for k,v in predicted_prev_token_distribution.items(): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch = [next(dialog_pretrain_data_loader.dataset.__iter__()) for i in range(0, batch_size)]\n",
    "batch_idx = 1\n",
    "batch = dialog_pretrain_data_loader.collate_fn(batch=_batch)\n",
    "batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "src_inputs, tgt_inputs, tgt_outputs = batch\n",
    "predictions = transformer(src_inputs=src_inputs, tgt_inputs=tgt_inputs)\n",
    "\n",
    "# loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "#                                         criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "#                                         train=True, amp=amp, scaler=scaler)\n",
    "\n",
    "# print(loss_dict)\n",
    "# print(acc_dict)\n",
    "# break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "_targets = prev_token_distribution\n",
    "_prediction = predictions[\"lm\"]\n",
    "prediction_token_ids = torch.argmax(_prediction, axis=-1)\n",
    "# prediction_token_ids = trainer.convert_to_numpy(tensor=prediction_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = prediction_token_ids[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14795"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prediction = _prediction.exp()\n",
    "for input_row, target_row in zip(_prediction, _targets):\n",
    "#     candidate_mask = target_row\n",
    "#     if not ngram_distribution:\n",
    "#         # make subsequent mask as a default\n",
    "#         target_expanded = target_row.unsqueeze(0).expand(self.timesteps, self.timesteps)\n",
    "#         target_tril = target_expanded.tril(-1)\n",
    "#         ignore_index_mask = target_expanded.triu().to(torch.bool) * self.ignore_index\n",
    "#         target_tril = target_tril + ignore_index_mask\n",
    "#         candidate_mask = target_tril.masked_fill(target_tril == target_row.unsqueeze(1), 0)\n",
    "\n",
    "    # clamp: prevent underflow\n",
    "    probs_not_to_be = torch.clamp(1 - input_row, min=1e-7)\n",
    "    # convert probabilities to log_probailities again\n",
    "    negatvie_candidates = torch.zeros_like(input_row).scatter_(1, target_row, 1)\n",
    "    _loss = -1 * torch.log(probs_not_to_be) * negatvie_candidates\n",
    "    _loss[:, ignore_index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,     0,     0, 12500,  8978, 14120,  4676,  8356,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_token_distribution[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14795, device='cuda:0')"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulk_targets = []\n",
    "empty_row = [0] * ngram\n",
    "for row in prediction_token_ids:\n",
    "    ulk_token_row = [most_different_tokens[token_id] if token_id in most_different_tokens else empty_row for token_id in row]\n",
    "    ulk_targets.append(ulk_token_row)\n",
    "ulk_targets = trainer.convert_to_tensor(data=ulk_targets, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-eddf52eecd4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprev_token_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprediction_token_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_prev_token_distribution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_token_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0m_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "for prev_token_id in prediction_token_ids:\n",
    "    distribution = predicted_prev_token_distribution[prev_token_id]\n",
    "    token_ids = list(distribution.keys())\n",
    "    frequencies = np.array(list(distribution.values()))\n",
    "    _frequencies = frequencies / sum(frequencies)\n",
    "    \n",
    "    dict(zip(token_ids, _frequencies))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14795, 12500, 12312,  ...,  3325, 14120, 14120],\n",
       "        [ 6906, 12500, 14469,  ..., 14120, 14120, 14120],\n",
       "        [  261,  3375, 14469,  ..., 14120,   830, 14120],\n",
       "        ...,\n",
       "        [ 9505, 12500, 12689,  ..., 14120, 14120,  3227],\n",
       "        [ 6734, 12500,  8025,  ...,   830, 14120, 14120],\n",
       "        [ 9505, 12500, 14469,  ..., 14120, 14120, 12274]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'Tagger' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-33c7ba0c846c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msrc_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_outputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdialog_pretrain_data_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msrc_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdialog_pretrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msrc_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtgt_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdialog_pretrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtgt_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtgt_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdialog_pretrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtgt_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    912\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'Tagger' object"
     ]
    }
   ],
   "source": [
    "for src_inputs, tgt_inputs, tgt_outputs in dialog_pretrain_data_loader:\n",
    "    if len(src_inputs[\"token\"]) < 1: continue\n",
    "    src_inputs = {k: dialog_pretrainer.convert_to_tensor(data=v, device=device) for k, v in src_inputs.items()}\n",
    "    tgt_inputs = {k: dialog_pretrainer.convert_to_tensor(data=v, device=device) for k, v in tgt_inputs.items()}\n",
    "    tgt_outputs = {k: dialog_pretrainer.convert_to_tensor(data=v, device=device) for k, v in tgt_outputs.items()}\n",
    "    break\n",
    "    \n",
    "output = model(src_inputs=src_inputs, tgt_inputs=tgt_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./temp/20210607_174705/epoch_1/\"\n",
    "# # save\n",
    "# BertTrainer.save(path=model_path, model=model, optimizer=optimizer)\n",
    "\n",
    "# load\n",
    "checkpoint = BertTrainer.load(path=model_path)\n",
    "model = checkpoint[\"model\"]\n",
    "optimizer = checkpoint[\"optimizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3d60645ba4ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#     print(outputs[\"nsp\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     cnt += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     if cnt > 10: break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for inputs, outputs in train_data_loader:\n",
    "#     print(outputs[\"nsp\"])\n",
    "#     cnt += 1\n",
    "#     if cnt > 10: break\n",
    "#     break\n",
    "    print(np.array(inputs[\"token_ids\"]).shape, np.array(inputs[\"segment_ids\"]).shape, np.array(outputs[\"nsp\"]).shape, np.array(outputs[\"mlm\"]).shape)\n",
    "    break\n",
    "        \n",
    "for n,i,o in zip(outputs[\"nsp\"], bert_prep.decode(inputs[\"token_ids\"]), bert_prep.decode(outputs[\"mlm\"])):\n",
    "    print(\"nsp:\", n)\n",
    "    print(\"input:\", i)\n",
    "    print(\"output:\", o)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-17T08:48:41.060Z"
    }
   },
   "outputs": [],
   "source": [
    "# src_sentences = {\"train\":[], \"valid\":[], \"test\":[]}\n",
    "# tgt_sentences = {\"train\":[], \"valid\":[], \"test\":[]}\n",
    "\n",
    "# for conv_id, group in train_df.groupby([\"conv_id\"]):\n",
    "#     utterances = group[\"utterance\"].tolist()\n",
    "#     group_length = len(group)\n",
    "#     for i in range(0, group_length-1):\n",
    "#         src_sentence = utterances[i]\n",
    "#         tgt_sentence = utterances[i+1]\n",
    "#         src_sentences[\"train\"].append(src_sentence)\n",
    "#         tgt_sentences[\"train\"].append(tgt_sentence)\n",
    "\n",
    "# for conv_id, group in valid_df.groupby([\"conv_id\"]):\n",
    "#     utterances = group[\"utterance\"].tolist()\n",
    "#     group_length = len(group)\n",
    "#     for i in range(0, group_length-1):\n",
    "#         src_sentence = utterances[i]\n",
    "#         tgt_sentence = utterances[i+1]\n",
    "#         src_sentences[\"valid\"].append(src_sentence)\n",
    "#         tgt_sentences[\"valid\"].append(tgt_sentence)\n",
    "\n",
    "# for conv_id, group in test_df.groupby([\"conv_id\"]):\n",
    "#     utterances = group[\"utterance\"].tolist()\n",
    "#     group_length = len(group)\n",
    "#     for i in range(0, group_length-1):\n",
    "#         src_sentence = utterances[i]\n",
    "#         tgt_sentence = utterances[i+1]\n",
    "#         src_sentences[\"test\"].append(src_sentence)\n",
    "#         tgt_sentences[\"test\"].append(tgt_sentence)\n",
    "\n",
    "# data_type = \"test\"\n",
    "# with open(file_path[\"empatheticdialogues\"][\"feed_data\"][data_type].format(dataset_dir=dataset_dir), \"w\", encoding=\"UTF-8\") as fp:\n",
    "#     for src_sentence, tgt_sentence in zip(src_sentences[\"train\"], tgt_sentences[\"train\"]):\n",
    "#         row = src_sentence + \"\\t\" + tgt_sentence + \"\\n\"\n",
    "#         fp.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5285,\n",
       " 4646,\n",
       " 4260,\n",
       " 6083,\n",
       " 3973,\n",
       " 6620,\n",
       " 4103,\n",
       " 3918,\n",
       " 2997,\n",
       " 3604,\n",
       " 4444,\n",
       " 5162,\n",
       " 4510,\n",
       " 2175,\n",
       " 1908,\n",
       " 567]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sentence = \"test 센텐텐스스입입니니다다.\"\n",
    "model.eval()\n",
    "model.inference(preprocessor=prep, src_sentence=src_sentence, device=None, method=\"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T08:05:15.775619Z",
     "start_time": "2021-05-17T08:05:15.582059Z"
    }
   },
   "outputs": [],
   "source": [
    "from_idx = 0\n",
    "batch_data = transformer_dataset.get_batch(from_idx=from_idx, to_idx=from_idx+batch_size, device=None)\n",
    "predictions = model.forward(batch_data[\"src_inputs\"], batch_data[\"tgt_inputs\"])\n",
    "torch.argmax(predictions, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:02:22.407880Z",
     "start_time": "2021-05-17T05:02:22.400848Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T01:57:56.088400Z",
     "start_time": "2021-05-17T01:57:56.031404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: torch.Size([16, 16, 8000]) torch.Size([16, 16])\n",
      "after: torch.Size([256, 8000]) torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(28.2944, dtype=torch.float64, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch_data in train_loader:\n",
    "    break\n",
    "    \n",
    "predictions = model(batch_data[\"src_inputs\"], batch_data[\"tgt_inputs\"])\n",
    "targets = batch_data[\"tgt_labels\"]\n",
    "lm_loss.get_loss(predictions=predictions, targets=targets\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    for batch_data in train_loader:\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        # Model computations\n",
    "        [...]\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_loss() missing 2 required positional arguments: 'predictions' and 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-731d0bba7344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_loss() missing 2 required positional arguments: 'predictions' and 'targets'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.98)\n",
       "    eps: 1e-05\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.criterions[\"lm\"].get_loss()\n",
    "model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for data in train_loader:\n",
    "    break\n",
    "\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        # Model computations\n",
    "        [...]\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "src_inputs = np.random.randint(low=0, high=src_vocab_size, size=(batch_size, src_timesteps))\n",
    "tgt_inputs = np.random.randint(low=0, high=tgt_vocab_size, size=(batch_size, tgt_timesteps))\n",
    "src_inputs = torch.from_numpy(src_inputs)\n",
    "tgt_inputs = torch.from_numpy(tgt_inputs)\n",
    "# src_inputs = src_inputs.to(device)\n",
    "# tgt_inputs = tgt_inputs.to(device)\n",
    "\n",
    "tgt_labels = []\n",
    "for i in range(0, batch_size):\n",
    "    row = [prep.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]] * tgt_timesteps\n",
    "    l = np.random.randint(low=0, high=tgt_timesteps)\n",
    "    for j in range(0, l):\n",
    "        row[j] = np.random.randint(low=len(prep.tgt_spm_tokenizer.special_token_dict), high=tgt_vocab_size)\n",
    "    tgt_labels.append(row)\n",
    "tgt_labels = np.array(tgt_labels)\n",
    "tgt_labels = torch.from_numpy(tgt_labels).contiguous().view(-1)\n",
    "# tgt_labels = tgt_labels.to(device)\n",
    "\n",
    "def dummy_get_batch(mask, approach):\n",
    "    output = dict()\n",
    "    output[\"src_inputs\"] = src_inputs\n",
    "    output[\"tgt_inputs\"] = tgt_inputs\n",
    "    output[\"tgt_labels\"] = tgt_labels\n",
    "    return output\n",
    "\n",
    "trainer.get_batch = dummy_get_batch\n",
    "batch_data = trainer.get_batch(mask=False, approach=\"igrnore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(30.3885, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "parameter: Parameter containing:\n",
      "tensor([[ 0.0007, -0.0181,  0.0168,  ..., -0.0046, -0.0427,  0.0172],\n",
      "        [-0.0270, -0.0181, -0.0124,  ...,  0.0088,  0.0038,  0.0095],\n",
      "        [-0.0252,  0.0149,  0.0377,  ...,  0.0115,  0.0098, -0.0456],\n",
      "        ...,\n",
      "        [-0.0165,  0.0137, -0.0111,  ..., -0.0247, -0.0099,  0.0249],\n",
      "        [-0.0220,  0.0086,  0.0368,  ...,  0.0049, -0.0242,  0.0098],\n",
      "        [-0.0356,  0.0149,  0.0124,  ..., -0.0130, -0.0306,  0.0397]],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "predictions = model(batch_data[\"src_inputs\"], batch_data[\"tgt_inputs\"])\n",
    "loss = trainer.loss_function_dict[\"lm\"](predictions, batch_data[\"tgt_labels\"])\n",
    "print(\"loss:\", loss)\n",
    "print(\"parameter:\", model.encoder.layers[num_encoder_layers-1].pwff_layer.f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.0064, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "parameter: Parameter containing:\n",
      "tensor([[ 0.0172, -0.0072,  0.0255,  ...,  0.0112, -0.0251,  0.0327],\n",
      "        [-0.0369, -0.0546, -0.0205,  ..., -0.0075,  0.0238,  0.0247],\n",
      "        [-0.0220,  0.0214,  0.0454,  ...,  0.0023,  0.0287, -0.0571],\n",
      "        ...,\n",
      "        [-0.0182,  0.0326,  0.0083,  ..., -0.0156, -0.0324,  0.0314],\n",
      "        [-0.0217, -0.0004,  0.0601,  ..., -0.0231, -0.0172, -0.0039],\n",
      "        [-0.0243,  0.0155,  0.0115,  ..., -0.0232, -0.0054,  0.0553]],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "predictions = model(batch_data[\"src_inputs\"], batch_data[\"tgt_inputs\"])\n",
    "loss = trainer.loss_function_dict[\"lm\"](predictions, batch_data[\"tgt_labels\"])\n",
    "print(\"loss:\", loss)\n",
    "print(\"parameter:\", model.encoder.layers[num_encoder_layers-1].pwff_layer.f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model_state_dict', 'optimizer', 'optimizer_state_dict', 'batch_data'])\n"
     ]
    }
   ],
   "source": [
    "PATH = './model/test/'\n",
    "# # save\n",
    "# batch_data = trainer.get_batch(mask=False, approach=\"igrnore\")\n",
    "# # torch.save(model, PATH + 'model.pt')  # 전체 모델 저장\n",
    "# if not os.path.isdir(PATH): os.mkdir(PATH)\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer': optimizer,\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'batch_data': batch_data\n",
    "# }, PATH+'all.tar')\n",
    "\n",
    "# load\n",
    "checkpoint = torch.load(PATH+\"all.tar\")\n",
    "print(checkpoint.keys())\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "# model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "optimizer = checkpoint[\"optimizer\"]\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.0087, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "parameter: Parameter containing:\n",
      "tensor([[ 0.0172, -0.0072,  0.0255,  ...,  0.0112, -0.0251,  0.0327],\n",
      "        [-0.0369, -0.0546, -0.0205,  ..., -0.0075,  0.0238,  0.0247],\n",
      "        [-0.0220,  0.0214,  0.0454,  ...,  0.0023,  0.0287, -0.0571],\n",
      "        ...,\n",
      "        [-0.0182,  0.0326,  0.0083,  ..., -0.0156, -0.0324,  0.0314],\n",
      "        [-0.0217, -0.0004,  0.0601,  ..., -0.0231, -0.0172, -0.0039],\n",
      "        [-0.0243,  0.0155,  0.0115,  ..., -0.0232, -0.0054,  0.0553]],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# predictions = model(batch_data[\"src_inputs\"], batch_data[\"tgt_inputs\"])\n",
    "predictions = model(checkpoint[\"batch_data\"][\"src_inputs\"], checkpoint[\"batch_data\"][\"tgt_inputs\"])\n",
    "loss = trainer.loss_function_dict[\"lm\"](predictions, checkpoint[\"batch_data\"][\"tgt_labels\"])\n",
    "print(\"loss:\", loss)\n",
    "print(\"parameter:\", model.encoder.layers[num_encoder_layers-1].pwff_layer.f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gpu_setting\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2, 3\"\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device(0)\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(3)\n",
    "# torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fd2951b82103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            model.cuda(args.gpu)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.cuda(args.gpu, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.4639,  0.0497, -0.2549,  ..., -0.2591,  1.0861,  0.1500],\n",
       "        [-0.3107,  0.4950, -2.7424,  ..., -1.0883,  1.3823,  0.6258],\n",
       "        [-1.1147, -0.8964, -0.6551,  ...,  1.2953, -0.4786, -0.5413],\n",
       "        ...,\n",
       "        [ 0.6452, -0.4104, -0.7140,  ..., -0.8958, -1.1715,  0.9950],\n",
       "        [ 1.8464,  0.7755,  1.3069,  ..., -0.5902, -0.6805,  0.2433],\n",
       "        [ 0.2092,  0.9304,  0.1618,  ..., -0.8570, -0.1002, -2.4730]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "a = copy.deepcopy(model.src_embedding_layer.token_embedding_layer.weight)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.6363,  1.2300, -1.5523,  ...,  0.1990, -0.3337,  0.5117],\n",
       "        [ 1.1525,  1.2044, -0.7870,  ...,  0.2773, -0.6321,  0.6474],\n",
       "        [-0.8847,  1.1973, -0.0119,  ..., -0.6822,  0.5145,  0.0370],\n",
       "        ...,\n",
       "        [ 0.3510,  1.3155,  0.4474,  ...,  1.0271,  0.0918, -0.4542],\n",
       "        [-1.2046, -1.0570, -0.8211,  ...,  0.3967, -0.2932, -0.1849],\n",
       "        [-0.7565, -0.5659, -1.2976,  ..., -0.2027, -0.2616,  0.7284]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = copy.deepcopy(model.tgt_embedding_layer.token_embedding_layer.weight)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0010,  0.0003,  0.0235,  ...,  0.0152,  0.0392,  0.0209],\n",
       "        [ 0.0052, -0.0196, -0.0027,  ..., -0.0016,  0.0207,  0.0317],\n",
       "        [-0.0008,  0.0257, -0.0094,  ...,  0.0165,  0.0052,  0.0043],\n",
       "        ...,\n",
       "        [-0.0060,  0.0060, -0.0131,  ...,  0.0077, -0.0048, -0.0066],\n",
       "        [ 0.0039,  0.0254, -0.0172,  ...,  0.0257, -0.0269,  0.0087],\n",
       "        [ 0.0379, -0.0153,  0.0181,  ..., -0.0191,  0.0176, -0.0002]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers[num_encoder_layers-1].mha_layer.out_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0447, -0.0400, -0.0359,  ...,  0.0119, -0.0061,  0.0235],\n",
       "        [-0.0030,  0.0002,  0.0044,  ...,  0.0094,  0.0155, -0.0033],\n",
       "        [-0.0093, -0.0082, -0.0192,  ..., -0.0090, -0.0152,  0.0343],\n",
       "        ...,\n",
       "        [ 0.0012,  0.0349,  0.0127,  ..., -0.0248,  0.0058,  0.0218],\n",
       "        [ 0.0101,  0.0152,  0.0048,  ...,  0.0061,  0.0144,  0.0031],\n",
       "        [-0.0039, -0.0151,  0.0045,  ..., -0.0449,  0.0122,  0.0017]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers[num_encoder_layers-1].pwff_layer.f1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "print('Started Training')\n",
    "model.train()\n",
    "for epoch in range(0, epochs):  # loop over the dataset multiple times\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    num_iters = trainer.get_num_iters()\n",
    "    for i in range(0,num_iters):  \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        batch_data = trainer.get_batch(mask=False, approach=\"ignore\")\n",
    "        predictions = model(src_inputs=batch_data[\"src_inputs\"], tgt_inputs=batch_data[\"tgt_inputs\"])\n",
    "        predictions_flatten = predictions.view(-1, predictions.size(-1))\n",
    "        tgt_labels_flatten = batch_data[\"tgt_labels\"].contiguous().view(-1)\n",
    "        # backward & optimize\n",
    "        batch_loss = lm_loss(predictions_flatten, tgt_labels_flatten)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "\n",
    "    # print statistics\n",
    "    print(\"[epoch: {epoch}] loss: {loss}\".format(epoch=epoch+1, loss=epoch_loss/num_iters))\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(0, epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-787006a18637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_function' is not defined"
     ]
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.functional.log_softmax(input=decoder_output, dim=-1)\n",
    "a = nn.functional.softmax(input=decoder_output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       dtype=torch.float64, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'transpose_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cbfd95d25daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_token_embed_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'transpose_'"
     ]
    }
   ],
   "source": [
    "torch.transpose(tgt_token_embed_weights, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_token_embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=4, microseconds=858459)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b-a).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    best_acc = 0\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print('==> Preparing data..')\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "    dataset_train = CIFAR10(root='../data', train=True, download=True, \n",
    "                            transform=transforms_train)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=args.batch_size, \n",
    "                              shuffle=True, num_workers=args.num_worker)\n",
    "\n",
    "    # there are 10 classes so the dataset name is cifar-10\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    print('==> Making model..')\n",
    "\n",
    "    net = pyramidnet()\n",
    "    net = net.to(device)\n",
    "    num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print('The number of parameters of model is', num_params)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, \n",
    "                          momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    train(net, criterion, optimizer, train_loader, device)\n",
    "            \n",
    "\n",
    "def train_on_single_device(self, model, loss_function, optimizer, device, verbose_per_batch=100):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_begin_time = datetime.now()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_begin_time = datetime.now()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        \n",
    "        batch_end_time = datetime.now()\n",
    "        batch_train_time = batch_end_time - batch_begin_time\n",
    "        \n",
    "        if batch_idx % verbose_per_batch == 0:\n",
    "            print('Epoch: [{}/{}]| loss: {:.4f} | acc: {:.4f} | batch time: {:.4f}s '.format(batch_idx, len(train_loader), train_loss/(batch_idx+1), acc, batch_train_time.seconds))\n",
    "    \n",
    "    train_end_time = datetime.now()\n",
    "    total_train_time = train_end_time - train_begin_time\n",
    "    print(\"Training time: {seconds}s\".format(seconds=total_train_time.seconds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_template = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1 / 5 ] | loss: 0.7123\n"
     ]
    }
   ],
   "source": [
    "print(\"Epoch: [{epoch:^3d}/{epochs:^3d}] | {loss_template} | {acc_template} | train_time: batch time: {:.2f}s\".format(epoch=1, epochs=5, loss=0.7123))\n",
    "      \n",
    "#       \"| loss: {:.4f} | acc: {:.4f} | batch time: {:.4f}s '.format(batch_idx, len(train_loader), train_loss/(batch_idx+1), acc, batch_train_time.seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# src_inputs = np.array([\n",
    "#     [1,2,3,4,5,0,0],\n",
    "#     [1,2,3,4,0,0,0],\n",
    "#     [1,2,3,4,0,0,0],\n",
    "#     [1,2,3,4,5,6,7],\n",
    "#     [1,2,3,4,5,6,0],\n",
    "# ])\n",
    "# # src_inputs = np.random.randint(low=0, high=1, size=(batch_size, src_timesteps))\n",
    "# src_inputs = torch.from_numpy(src_inputs)\n",
    "# src_embedding_layer = TransformerEmbedding(timesteps=7, d_model=4, vocab_size=8)\n",
    "# src_embed, src_token_embed_weights = src_embedding_layer(token_ids=src_inputs)\n",
    "# src_key_padding_mask = src_inputs==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"kor\"\n",
    "dataset_name = \"open_subtitles\"\n",
    "\n",
    "data_dir = file_path[\"open_subtitles\"][\"dir\"].format(dataset_dir=dataset_dir)+\"/\" + language +\"/v2018/\"\n",
    "filenames = os.listdir(data_dir)\n",
    "\n",
    "test = []\n",
    "data_type = \"test\"\n",
    "for filename in filenames:\n",
    "    if not filename.startswith(data_type): continue\n",
    "    data = None\n",
    "    with open(data_dir+filename, \"r\", encoding=\"utf-8\") as fp:\n",
    "        data = fp.read()\n",
    "        data = data.strip()\n",
    "        if not data.startswith(\"[\") and not data.endswith(\"]\"):\n",
    "            data = \"[\" + data + \"]\"\n",
    "        data = data.replace(\"\\n\", \",\")\n",
    "        data = json.loads(data)\n",
    "    test += data\n",
    "\n",
    "train = []\n",
    "data_type = \"train\"\n",
    "for filename in filenames:\n",
    "    if not filename.startswith(data_type): continue\n",
    "    data = None\n",
    "    with open(data_dir+filename, \"r\", encoding=\"utf-8\") as fp:\n",
    "        data = fp.read()\n",
    "        data = data.strip()\n",
    "        if not data.startswith(\"[\") and not data.endswith(\"]\"):\n",
    "            data = \"[\" + data + \"]\"\n",
    "        data = data.replace(\"\\n\", \",\")\n",
    "        data = json.loads(data)\n",
    "    train += data\n",
    "\n",
    "len(train), len(test)\n",
    "dataset = {\"train\":train, \"test\":test}\n",
    "\n",
    "with open(file_path[dataset_name][\"pickle\"].format(dataset_dir=dataset_dir, language=language), \"wb\") as fp:\n",
    "    pickle.dum(dataset, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation - open_subtitles\n",
    "# sequences 중 약 8%에 전문 영어인 sequence가 있음\n",
    "with open(file_path[dataset_name][\"pickle\"].format(dataset_dir=dataset_dir, language=language), \"rb\") as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "def extract_sequence_from_row(row):\n",
    "    sequence = [None] * (len(row) - 1) # file_id\n",
    "    file_id = row.pop(\"file_id\")\n",
    "    context = row.pop(\"context\")\n",
    "    response = row.pop(\"response\")\n",
    "\n",
    "    sequence[0] = response\n",
    "    sequence[1] = context\n",
    "    for k,v in row.items():\n",
    "        v = v.strip()\n",
    "        if v == \"nbsp;\": continue\n",
    "        sequence[int(k[-1])+2] = v\n",
    "    sequence.reverse()\n",
    "    sequence = [sentence for sentence in sequence if sentence is not None]\n",
    "    return sequence\n",
    "\n",
    "train_sequences = []\n",
    "for row in dataset[\"train\"]:\n",
    "    sequence = extract_sequence_from_row(row=row)\n",
    "    if len(sequence) < 2: continue\n",
    "    train_sequences.append(sequence)\n",
    "\n",
    "test_sequences = []\n",
    "for row in dataset[\"test\"]:\n",
    "    sequence = extract_sequence_from_row(row=row)\n",
    "    if len(sequence) < 2: continue\n",
    "    test_sequences.append(sequence)\n",
    "    \n",
    "total_sequences = train_sequences + test_sequences\n",
    "sentences = [sentence for sequence in total_sequences for sentence in sequence]\n",
    "\n",
    "print(len(train_sequences), len(test_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512/2560/16/10/10 -> 47m\n",
    "512/2560/32/10/10 -> 47m\n",
    "512/3072/16/10/10 -> 47m\n",
    "512/3072/32/10/10 -> 47m\n",
    "\n",
    "512/2560/16/12/12 -> 53m *\n",
    "512/2560/32/12/12 -> 53m\n",
    "512/3072/16/12/12 -> 53m\n",
    "512/3072/32/12/12 -> 53m\n",
    "\n",
    "512/2560/16/2/12 -> 42m\n",
    "512/2560/32/2/12 -> 42m\n",
    "512/3072/16/2/12 -> 42m\n",
    "512/3072/32/2/12 -> 42m\n",
    "\n",
    "512/2560/16/2/24 -> 68m # \n",
    "512/2560/32/2/24 -> 68m\n",
    "512/3072/16/2/24 -> 68m\n",
    "512/3072/32/2/24 -> 68m\n",
    "\n",
    "768/2560/16/10/10 -> 94m\n",
    "768/2560/32/10/10 -> 94m\n",
    "768/3072/16/10/10 -> 94m\n",
    "768/3072/32/10/10 -> 94m\n",
    "\n",
    "768/2560/16/12/12 -> 108m\n",
    "768/2560/32/12/12 -> 108m\n",
    "768/3072/16/12/12 -> 108m\n",
    "768/3072/32/12/12 -> 108m\n",
    "\n",
    "768/2560/16/2/12 -> 84m\n",
    "768/2560/32/2/12 -> 84m\n",
    "768/3072/16/2/12 -> 84m\n",
    "768/3072/32/2/12 -> 84m\n",
    "\n",
    "768/2560/16/2/24 -> 140m\n",
    "768/2560/32/2/24 -> 140m\n",
    "768/3072/16/2/24 -> 140m\n",
    "768/3072/32/2/24 -> 140m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnlikelyhoodLoss Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from transformer.trainer.criterions import UnlikelihoodCriterion\n",
    "from transformer.trainer.custom_loss import UnlikelihoodLoss, UnlikelihoodLoss2d\n",
    "\n",
    "def generate_sample(batch_size, timesteps, vocab_size):\n",
    "    predictions = torch.rand((batch_size, timesteps, vocab_size), dtype=torch.double)\n",
    "    predictions = torch.log_softmax(predictions, dim=-1)\n",
    "    targets = torch.randint(low=0, high=vocab_size, size=(batch_size, timesteps))\n",
    "#     predictions = predictions.view(-1, predictions.size(-1))\n",
    "#     targets = targets.contiguous().view(-1)\n",
    "    return predictions, targets\n",
    "\n",
    "batch_size = 64\n",
    "timesteps_list = [8, 16, 32, 64, 128, 256]\n",
    "vocab_size_list = [32, 64, 8000, 15000, 30000]\n",
    "ignore_index = 0 \n",
    "underflow = 1e-5\n",
    "\n",
    "output = []\n",
    "for timesteps in timesteps_list:\n",
    "    for vocab_size in vocab_size_list:\n",
    "        if batch_size * timesteps >= vocab_size: continue\n",
    "        ulf_1 = UnlikelihoodLoss(batch_size=batch_size, timesteps=timesteps, vocab_size=vocab_size, ignore_index=ignore_index, underflow=underflow)\n",
    "        ulf_2 = UnlikelihoodLoss2d(timesteps=timesteps, vocab_size=vocab_size, ignore_index=ignore_index, underflow=underflow)\n",
    "        \n",
    "        predictions, targets = generate_sample(batch_size, timesteps, vocab_size)\n",
    "        print(\"timesteps:{t}, vocab_size:{v}\\tpredictions:{p}, targets:{g}\".format(t=timesteps, v=vocab_size, p=predictions.shape, g=targets.shape))\n",
    "        \n",
    "        begin = datetime.now()\n",
    "        loss_1 = ulf_1(predictions.view(-1, predictions.size(-1)), targets.contiguous().view(-1))\n",
    "        end = datetime.now()\n",
    "        exec_time_1 = (end - begin).seconds\n",
    "        \n",
    "        begin = datetime.now()\n",
    "        loss_2 = ulf_2(predictions, targets)\n",
    "        end = datetime.now()\n",
    "        exec_time_2 = (end - begin).seconds\n",
    "        \n",
    "        _output = {\"type\": \"1d\", \"batch_size\": batch_size, \"timesteps\": timesteps, \"vocab_size\": vocab_size, \"loss\": loss_1.item(), \"exec_time\": exec_time_1}\n",
    "        output.append(_output)\n",
    "        _output = {\"type\": \"2d\", \"batch_size\": batch_size, \"timesteps\": timesteps, \"vocab_size\": vocab_size, \"loss\": loss_2.item(), \"exec_time\": exec_time_2}\n",
    "        output.append(_output)\n",
    "        \n",
    "output_df = pd.DataFrame(output)\n",
    "for group in output_df.groupby([\"timesteps\", \"vocab_size\"]):\n",
    "    print(group)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
