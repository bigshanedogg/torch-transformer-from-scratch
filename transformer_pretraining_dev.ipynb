{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"Hodong_Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T02:55:10.811070Z",
     "start_time": "2021-06-15T02:55:09.470506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer.preprocessors.blender_bot_preprocessor import GeneratorPretrainingPreprocessor\n",
    "from transformer.data.dataset import DatasetInterface, DatasetFromDir\n",
    "from transformer.data.blender_bot_data_loader import GeneratorPretrainingDataLoader\n",
    "from transformer.models.transformer import Transformer\n",
    "from transformer.trainers.blender_bot_trainer import GeneratorPretrainingTransformerTrainer\n",
    "from transformer.trainers.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T02:55:10.856060Z",
     "start_time": "2021-06-15T02:55:10.812801Z"
    }
   },
   "outputs": [],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Picas_Server\n",
    "# dataset_dir = \"/home/picas/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# Korea_Server\n",
    "dataset_dir = \"/home/mnt/guest1\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)\n",
    "\n",
    "# # bigshane_local\n",
    "# dataset_dir = \"D:\\_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # AWS\n",
    "# dataset_dir = \"/home/ubuntu/data\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./scripts/transformer/config/generator_finetuning_korea.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported konlpy.tag.Mecab successfully\n",
      "loaded spm_model: '/Users/aibud_dev/_jupyter/spm_model/kor/spoken_pretrain_spm_v30000/'\n"
     ]
    }
   ],
   "source": [
    "src_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=config[\"data\"][\"src_language\"], vocab_size=config[\"model\"][\"src_vocab_size\"])\n",
    "tgt_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=config[\"data\"][\"tgt_language\"], vocab_size=config[\"model\"][\"tgt_vocab_size\"])\n",
    "# src_spm_model_path = config[\"data\"][\"src_spm_model_path\"].format(root_dir=config[\"data\"][\"root_dir\"], language=config[\"data\"][\"src_language\"], vocab_size=config[\"model\"][\"src_vocab_size\"])\n",
    "# tgt_spm_model_path = config[\"data\"][\"tgt_spm_model_path\"].format(root_dir=config[\"data\"][\"root_dir\"], language=config[\"data\"][\"tgt_language\"], vocab_size=config[\"model\"][\"tgt_vocab_size\"])\n",
    "preprocessor = GeneratorPretrainingPreprocessor(src_language=config[\"data\"][\"src_language\"], tgt_language=config[\"data\"][\"tgt_language\"], src_spm_model_path=src_spm_model_path, tgt_spm_model_path=tgt_spm_model_path, embedding_dict=config[\"model\"][\"embedding_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'temp_dir' has been set to '/Users/aibud_dev/_jupyter/model/temp/20210827_174129/' to save model while training\n"
     ]
    }
   ],
   "source": [
    "trainer = GeneratorPretrainingTransformerTrainer(temp_dir=dataset_dir+\"/model/temp/\")\n",
    "# trainer = GeneratorPretrainingTransformerTrainer(temp_dir=config[\"train\"][\"temp_save_path\"])\n",
    "# trainer.set_lr_update(initial_learning_rate=config[\"optimizer\"][\"initial_learning_rate\"], num_warmup_steps=config[\"train\"][\"num_warmup_steps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(src_pad_token_id=preprocessor.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], tgt_pad_token_id=preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], **config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = dataset_dir + \"/model/transformer/dialog_pretrain/20210821/epoch_100/\"\n",
    "# model_dir = dataset_dir + \"/model/temp/20210826_104849/epoch_140/\"\n",
    "transformer = load_state_dict(object=transformer, path=model_dir + ModelFilenameConstants.MODEL_STATE_DICT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set criterions & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"optimizer\"][\"initial_learning_rate\"] = 1e-4\n",
    "criterions, criterion_weights = trainer.get_criterions(tgt_timesteps=config[\"model\"][\"tgt_timesteps\"], tgt_vocab_size=config[\"model\"][\"tgt_vocab_size\"], tgt_pad_token_id=preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], **config[\"criterion\"])\n",
    "optimizer = trainer.get_optimizer(model=transformer, **config[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model device: cpu\n",
      "Setting criterions device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = GeneratorPretrainingTransformerTrainer.set_device(obj=transformer, device=device)\n",
    "optimizer = GeneratorPretrainingTransformerTrainer.set_device(obj=optimizer, device=device)\n",
    "criterions = GeneratorPretrainingTransformerTrainer.set_device(obj=criterions, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53aaade5ecf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtotal_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msample_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/sample/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/train/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# data_loader_params\n",
    "batch_size = 4 # 96\n",
    "nprocs = 1\n",
    "\n",
    "total_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/\"\n",
    "sample_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/sample/\"\n",
    "train_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/train/\"\n",
    "val_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/val/\"\n",
    "\n",
    "train_dataset = DatasetFromDir(data_dir=train_data_dir, batch_size=batch_size, device=device, nprocs=nprocs, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"])\n",
    "train_data_loader_params = trainer.get_data_loader_params(dataset=train_dataset, preprocessor=preprocessor, batch_size=batch_size, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "train_data_loader = trainer.create_data_loader(**train_data_loader_params)\n",
    "\n",
    "val_dataset = DatasetFromDir(data_dir=val_data_dir, batch_size=batch_size, device=device, nprocs=nprocs, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"])\n",
    "val_data_loader_params = trainer.get_data_loader_params(dataset=val_dataset, preprocessor=preprocessor, batch_size=batch_size, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "val_data_loader = trainer.create_data_loader(**val_data_loader_params)\n",
    "\n",
    "# ngram = 5\n",
    "# utterances = [utterance for row in val_dataset.get_all_data() for utterance in row[\"utterances\"]]\n",
    "# target_prev_token_distribution, special_token_ids = preprocessor.extract_prev_token_distribution(sentences=utterances, ngram=ngram)\n",
    "# trainer.set_prev_token_distribution(prev_token_distribution=target_prev_token_distribution, special_token_ids=special_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_loader.summary(show_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader encode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_idx = 2\n",
    "# src_inputs, tgt_inputs, tgt_outputs = train_data_loader.get_batch()\n",
    "\n",
    "# print(\"src_input_token:\\t\", [token_idx for token_idx in range(0, len(src_inputs[\"token\"][row_idx])) if token_idx==0 or src_inputs[\"token\"][row_idx][token_idx]==preprocessor.src_spm_tokenizer.special_token_dict[\"sep\"][\"id\"]])\n",
    "# print(\"src_input_segment:\\t\", [token_idx for token_idx in range(0, len(src_inputs[\"segment\"][row_idx])-1) if token_idx==0 or src_inputs[\"segment\"][row_idx][token_idx]!=src_inputs[\"segment\"][row_idx][token_idx+1]])\n",
    "# # print(\"src_input_turn:\\t\", [token_idx for token_idx in range(0, len(src_inputs[\"turn\"][row_idx])-1) if token_idx==0 or src_inputs[\"turn\"][row_idx][token_idx]!=src_inputs[\"turn\"][row_idx][token_idx+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "amp = True\n",
    "scaler = None\n",
    "if amp: scaler = torch.cuda.amp.GradScaler()\n",
    "save_per_epoch = 1\n",
    "save_per_batch = -1\n",
    "keep_last = True\n",
    "verbose_per_epoch = 1\n",
    "verbose_per_batch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.fit(model=transformer, train_data_loader=train_data_loader, val_data_loader=None, \n",
    "                      criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                      epoch=epoch, amp=amp, save_per_epoch=save_per_epoch, save_per_batch=save_per_batch, keep_last=keep_last, verbose_per_epoch=verbose_per_epoch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = tqdm(train_data_loader, initial=train_data_loader.iter_start, total=len(train_data_loader))\n",
    "data_iter.iter_size = train_data_loader.iter_end - train_data_loader.iter_start\n",
    "epoch_train_history = trainer.train_epoch(model=transformer, data_loader=data_iter, \n",
    "                                          criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                                          amp=amp, scaler=scaler, save_per_batch=save_per_batch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-db153710a2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mbatch_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_data_loader):\n",
    "    batch_idx += 1\n",
    "    batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "    \n",
    "    loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "                                            criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                            train=True, amp=amp, scaler=scaler)\n",
    "    \n",
    "    print(loss_dict)\n",
    "    print(acc_dict)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration & data_loader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Int for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-138db4f95a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                         \u001b[0mcriterions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                         train=True, amp=amp, scaler=scaler)\n",
      "\u001b[1;32mD:\\_jupyter\\torch-transformer\\transformer\\trainer\\transformer_trainer.py\u001b[0m in \u001b[0;36miteration\u001b[1;34m(self, model, batch, criterions, criterion_weights, optimizer, train, amp, scaler)\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[0m_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                 \u001b[0m_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0m_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0m_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_targets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\_jupyter\\torch-transformer\\transformer\\trainer\\criterions.py\u001b[0m in \u001b[0;36mget_loss\u001b[1;34m(self, predictions, targets)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         )\n\u001b[0;32m   2387\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2388\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2389\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Int for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "_batch = [next(train_data_loader.dataset.__iter__()) for i in range(0, batch_size)]\n",
    "batch_idx = 1\n",
    "batch = train_data_loader.collate_fn(batch=_batch)\n",
    "batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "\n",
    "loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "                                        criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                        train=True, amp=amp, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_params = trainer.get_data_loader_params(dataset=train_dataset, preprocessor=preprocessor, batch_size=1, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "train_data_loader = trainer.create_data_loader(**train_data_loader_params)\n",
    "val_data_loader_params = trainer.get_data_loader_params(dataset=val_dataset, preprocessor=preprocessor, batch_size=1, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "val_data_loader = trainer.create_data_loader(**val_data_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def loader_iter(data_loader):\n",
    "    for batch in data_loader:\n",
    "        yield batch\n",
    "\n",
    "train_gen = loader_iter(train_data_loader)\n",
    "val_gen = loader_iter(val_data_loader)\n",
    "\n",
    "transformer.eval()\n",
    "src_pad_token_id = preprocessor.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]\n",
    "tgt_pad_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]\n",
    "tgt_bos_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"speaker_1\"][\"id\"]\n",
    "tgt_eos_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"eos\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = train_gen\n",
    "src_inputs = {\"token\":[]}\n",
    "while len(src_inputs[\"token\"]) < 1:\n",
    "    batch = next(gen)\n",
    "    src_inputs, tgt_inputs, tgt_outputs = batch\n",
    "\n",
    "context = preprocessor.src_decode(src_inputs[\"token\"])[0]\n",
    "greedy_prediction = transformer.inference_greedy(src_inputs=src_inputs, src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, tgt_bos_token_id=tgt_bos_token_id, tgt_eos_token_id=tgt_eos_token_id)\n",
    "greedy_reply = preprocessor.tgt_decode(greedy_prediction)[0]\n",
    "beam_prediction, probs = transformer.inference_beam_search(src_inputs=src_inputs, src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, tgt_bos_token_id=tgt_bos_token_id, tgt_eos_token_id=tgt_eos_token_id)\n",
    "beam_replies = preprocessor.tgt_decode(beam_prediction)\n",
    "sampling_prediction, probs = transformer.inference_random_sampling(src_inputs=src_inputs, src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, tgt_bos_token_id=tgt_bos_token_id, tgt_eos_token_id=tgt_eos_token_id, num_samples=5, temperature=0.7)\n",
    "sampling_replies = preprocessor.tgt_decode(sampling_prediction)\n",
    "\n",
    "ctxt_list = re.split(\"(<spk1>|<spk2>)\", context)[1:]\n",
    "for i in range(0, len(ctxt_list), 2):\n",
    "    print(\"{}: {}\".format(ctxt_list[i], ctxt_list[i+1]))\n",
    "print(\"{}: {}\".format(\"<spk1>(greedy)\", greedy_reply))\n",
    "for beam_reply in beam_replies:\n",
    "    print(\"{}: {}\".format(\"<spk1>(beam)\", beam_reply))\n",
    "print(\"{}: {}\".format(\"<spk1>(sampling)\", sampling_replies[0]))\n",
    "print(\"({}: {})\".format(\"ans\", preprocessor.tgt_decode(tgt_outputs[\"lm\"])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_dir = trainer.temp_dir + \"epoch_13/\"\n",
    "# trainer.save(path=_model_dir, model=transformer, optimizer=optimizer, history=None, config=config, preprocessor=preprocessor, save_model_hyperparams=True, save_optimizer_hyperparams=False, ddp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.services.dialog_generator.transformer import DialogGenerator\n",
    "dg = DialogGenerator(temp_dir=\"./\")\n",
    "dg.load_model(model_dir=_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [\n",
    "    \"여기 있는 사람들이 다 롤러코스터 타려고 기다리는 사람들이야?\",\n",
    "    \"그런 것 같아요.\",\n",
    "    \"얼마나 기다려야 할까?\",\n",
    "    \"최소한 한 시간 반 정도 기다려야 될 것 같아요.\",\n",
    "    \"한 시간 반?\",\n",
    "    \"줄이 너무 기니까 우리 다른 것부터 탈까?\"\n",
    "]\n",
    "speaker_ids = [1, 0, 1, 0, 1, 1]\n",
    "conditions = None # [\"condition 문장입니다.\"]\n",
    "beam_size = 5\n",
    "min_length = 5\n",
    "lp_alpha = 1.2\n",
    "lp_min_length = 5\n",
    "return_probs = False\n",
    "max_retry = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy\n",
    "dg.infer_next_utterance_greedy(utterances=utterances, speaker_ids=speaker_ids, conditions=conditions, max_retry=max_retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search\n",
    "dg.infer_next_utterance_beam_search(utterances=utterances, speaker_ids=speaker_ids, conditions=conditions,\n",
    "                                    beam_size=beam_size, min_length=min_length, lp_alpha=lp_alpha, lp_min_length=lp_min_length, return_probs=return_probs,\n",
    "                                    max_retry=max_retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
