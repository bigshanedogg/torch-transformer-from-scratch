{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:20:51.097290Z",
     "start_time": "2021-05-25T16:20:49.794812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup, element\n",
    "\n",
    "from transformer.utils.tokenizer import MecabTokenizer, SpmTokenizer\n",
    "from transformer.data.dataset import DatasetInterface, DatasetFromDir\n",
    "from transformer.preprocessors.bert_preprocessor import BertPreprocessor\n",
    "from transformer.preprocessors.blender_bot_preprocessor import GeneratorPretrainingPreprocessor\n",
    "from transformer.preprocessors.utils import split_segment_by_speaker_ids, convert_turn_ids, flatten_sequence\n",
    "from transformer.utils.common import get_nth_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"무슨 일로 오셨나요?\"\n",
    "B = \"무슨 고민이 있어 오셨어요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spm_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f1795f445d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtoken_ids_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubtokens_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspm_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdToPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubtokens_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspm_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdToPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spm_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "token_ids = spm_tokenizer.tokens_to_ids(tokens=tokens_a, mask=False)\n",
    "token_ids_b = spm_tokenizer.tokens_to_ids(tokens=tokens_b, mask=False)\n",
    "subtokens_a = [spm_tokenizer.IdToPiece(token_id) for token_id in token_ids]\n",
    "subtokens_a = [spm_tokenizer.IdToPiece(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtoken_pieces_a = preprocessor.src_spm_tokenizer.tokens_to_pieces(tokens=[\"test\", \"문장\", \"입니다.\"], mask=False)\n",
    "# subtoken_pieces_b = spm_tokenizer.tokens_to_pieces(tokens=tokens_b, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁t', 'e', 'st', '▁문', '장', '▁입니다', '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtoken_pieces_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.03125e-06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0009 ** 1/128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIBUD_DEV\n",
    "dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)\n",
    "\n",
    "# # Picas_Server\n",
    "# dataset_dir = \"/home/picas/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Korea_Server\n",
    "# dataset_dir = \"/home/mnt/guest1\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # bigshane_local\n",
    "# dataset_dir = \"D:\\_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x):\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def trim_obj(obj):\n",
    "    obj = list(set(obj))\n",
    "    obj = [e for e in obj if e!=\"\"]\n",
    "    obj = sorted(obj)\n",
    "    return obj\n",
    "    \n",
    "def show_five_nums(data):\n",
    "    quartiles = np.percentile(data, [25, 50, 75])\n",
    "    min_v = np.min(data)\n",
    "    max_v = np.max(data)\n",
    "    avg = np.mean(data)\n",
    "    print(\"Min: {min_v:.3f}\\tMax: {max_v:.3f}\\tAvg: {avg:.3f}\\tQ1: {q1:.3f}\\tQ2: {q2:.3f}\\tQ3: {q3:.3f}\".format(min_v=min_v, max_v=max_v, avg=avg, q1=quartiles[0], q2=quartiles[1], q3=quartiles[2]))\n",
    "    return min_v, max_v, quartiles[0], quartiles[1], quartiles[2], avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported konlpy.tag.Mecab successfully\n",
      "loaded spm_model: '/Users/aibud_dev/_jupyter/spm_model/kor/spoken_pretrain_spm_v15000/'\n"
     ]
    }
   ],
   "source": [
    "src_language = \"kor\"\n",
    "tgt_language = \"kor\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "src_vocab_size = tgt_vocab_size = 15000\n",
    "embedding_dict = {\n",
    "    \"segment\": 2, # context, condition\n",
    "}\n",
    "src_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=src_language, vocab_size=src_vocab_size)\n",
    "tgt_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=tgt_language, vocab_size=tgt_vocab_size)\n",
    "preprocessor = GeneratorPretrainingPreprocessor(src_language=src_language, tgt_language=tgt_language, src_spm_model_path=src_spm_model_path, tgt_spm_model_path=tgt_spm_model_path, embedding_dict=embedding_dict)\n",
    "src_pad_token_id = preprocessor.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]\n",
    "tgt_pad_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wriet as Json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(root_dir + \"/{language}/multi_turn/feed_data_0.json\".format(language=language), \"w\", encoding=encoding) as fp:\n",
    "#     json.dump(output, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><hr><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dialog pretrain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = [\"KLUE_DST\", \"NIKL_2020\", \"NIKL_Dialog\", \"AIHUB_Twitter\", \"AIHUB_SSGI\", \"AIHUB_KETI\", \"AIHUB_SNS\", \"AIHUB_EMOTION\", \"KaggleConversation\", \"OpenSubtitles\"]\n",
    "output_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn/\"\n",
    "# dataset_name_list = [\"KLUE_DST\", \"AIHUB_Twitter\", \"AIHUB_SSGI\", \"AIHUB_KETI\", \"AIHUB_SNS\", \"AIHUB_EMOTION\", \"NIKL_2020\", \"NIKL_Dialog\", \"KaggleConversation\"]\n",
    "# output_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn_v2/\"\n",
    "# dataset_name_list = [\"KLUE_DST\", \"AIHUB_Twitter\", \"AIHUB_SSGI\", \"AIHUB_KETI\", \"AIHUB_SNS\", \"AIHUB_EMOTION\"]\n",
    "# output_dir = dataset_dir + \"/dataset/preprocessed/dialog_pretrain/kor/multi_turn_v3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load data from datasets...\n",
      "\t KLUE_DST : 9000 \t\tavg(utt_size): 15.0\n",
      "\t NIKL_2020 : 2232 \t\tavg(utt_size): 66.0\n",
      "\t NIKL_Dialog : 102 \t\tavg(utt_size): 510.0\n",
      "\t AIHUB_Twitter : 1999 \t\tavg(utt_size): 7.0\n",
      "\t AIHUB_SSGI : 5855 \t\tavg(utt_size): 15.0\n",
      "\t AIHUB_KETI : 4882 \t\tavg(utt_size): 9.0\n",
      "\t AIHUB_KETI : 5337 \t\tavg(utt_size): 6.0\n",
      "\t AIHUB_KETI : 749 \t\tavg(utt_size): 7.0\n",
      "\t AIHUB_SNS : 36680 \t\tavg(utt_size): 11.0\n",
      "\t AIHUB_EMOTION : 84212 \t\tavg(utt_size): 5.0\n",
      "\t KaggleConversation : 849 \t\tavg(utt_size): 5.0\n",
      "\t OpenSubtitles : 1023834 \t\tavg(utt_size): 12.0\n",
      "\n",
      "# Extract utterances from data...\n",
      "\t KLUE_DST : 28272\n",
      "\t NIKL_2020 : 6811\n",
      "\t NIKL_Dialog : 13189\n",
      "\t AIHUB_Twitter : 2900\n",
      "\t AIHUB_SSGI : 17805\n",
      "\t AIHUB_KETI : 12575\n",
      "\t AIHUB_SNS : 35889\n",
      "\t AIHUB_EMOTION : 84212\n",
      "\t KaggleConversation : 849\n",
      "\t OpenSubtitles : 1782125\n",
      "\n",
      "# Write extracted dialogs...\n",
      "\t feed_data_00.json : 100000\n",
      "\t feed_data_01.json : 100000\n",
      "\t feed_data_02.json : 100000\n",
      "\t feed_data_03.json : 100000\n",
      "\t feed_data_04.json : 100000\n",
      "\t feed_data_05.json : 100000\n",
      "\t feed_data_06.json : 100000\n",
      "\t feed_data_07.json : 100000\n",
      "\t feed_data_08.json : 100000\n",
      "\t feed_data_09.json : 100000\n",
      "\t feed_data_10.json : 100000\n",
      "\t feed_data_11.json : 100000\n",
      "\t feed_data_12.json : 100000\n",
      "\t feed_data_13.json : 100000\n",
      "\t feed_data_14.json : 100000\n",
      "\t feed_data_15.json : 100000\n",
      "\t feed_data_16.json : 100000\n",
      "\t feed_data_17.json : 100000\n",
      "\t feed_data_18.json : 100000\n",
      "\t feed_data_19.json : 84627\n",
      "\ttrain: feed_data_00.json : 100000\n",
      "\ttrain: feed_data_01.json : 100000\n",
      "\ttrain: feed_data_02.json : 100000\n",
      "\ttrain: feed_data_03.json : 100000\n",
      "\ttrain: feed_data_04.json : 100000\n",
      "\ttrain: feed_data_05.json : 100000\n",
      "\ttrain: feed_data_06.json : 100000\n",
      "\ttrain: feed_data_07.json : 100000\n",
      "\ttrain: feed_data_08.json : 100000\n",
      "\ttrain: feed_data_09.json : 100000\n",
      "\ttrain: feed_data_10.json : 100000\n",
      "\ttrain: feed_data_11.json : 100000\n",
      "\ttrain: feed_data_12.json : 100000\n",
      "\ttrain: feed_data_13.json : 100000\n",
      "\ttrain: feed_data_14.json : 100000\n",
      "\ttrain: feed_data_15.json : 100000\n",
      "\ttrain: feed_data_16.json : 86932\n",
      "\tval: feed_data_00.json : 100000\n",
      "\tval: feed_data_01.json : 100000\n",
      "\tval: feed_data_02.json : 97695\n"
     ]
    }
   ],
   "source": [
    "min_num_turns = 4\n",
    "max_num_turns = 8\n",
    "shuffle = True\n",
    "train_ratio = 0.85\n",
    "num_samples = 5000\n",
    "\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "print(\"# Load data from datasets...\")\n",
    "stat_dict = OrderedDict()\n",
    "data = OrderedDict()\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "    if \"multi_turn\" not in file_path[dataset_name][\"feed_data\"]: continue\n",
    "\n",
    "    data_path = file_path[dataset_name][\"feed_data\"][\"multi_turn\"].format(root_dir=root_dir, language=language)\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if not file_name.endswith(extension): continue\n",
    "        with open(data_path+file_name, \"r\", encoding=encoding) as fp:\n",
    "            _data = json.load(fp)\n",
    "\n",
    "            if dataset_name not in data: data[dataset_name] = []\n",
    "            data[dataset_name] += _data\n",
    "            _avg_utt_size = np.round(np.mean([len(r[\"utterances\"]) for r in _data]))\n",
    "            print(\"\\t\", dataset_name, \":\", len(_data), \"\\t\\tavg(utt_size):\", _avg_utt_size)\n",
    "            stat_dict[dataset_name] = dict()\n",
    "            stat_dict[dataset_name][\"dataset_size\"] = len(_data)\n",
    "            stat_dict[dataset_name][\"avg_utterances\"] = _avg_utt_size\n",
    "print()\n",
    "\n",
    "print(\"# Extract utterances from data...\")\n",
    "output = []\n",
    "train_output = []\n",
    "val_output = []\n",
    "\n",
    "for dataset_name, _data in data.items():\n",
    "    _train_rows = []\n",
    "    _val_rows = []\n",
    "    \n",
    "    train_idx = int(len(_data) * train_ratio)\n",
    "    for _row_idx, _row in enumerate(_data):\n",
    "        _num_turns = np.random.randint(low=min_num_turns, high=max_num_turns)\n",
    "        sequence_utterances, sequence_turn_ids = split_segment_by_speaker_ids(utterances=_row[\"utterances\"], speaker_ids=_row[\"speaker_ids\"])\n",
    "        num_speakers = len(set([turn_id for segment_turn_id in sequence_turn_ids for turn_id in segment_turn_id]))\n",
    "        if num_speakers != 2: continue\n",
    "\n",
    "        turn_iter = max(len(sequence_utterances) - _num_turns, 0)\n",
    "        for i in range(0, turn_iter+1): \n",
    "            _sequence_utterances = sequence_utterances[i:i+_num_turns]\n",
    "            _sequence_turn_ids = sequence_turn_ids[i:i+_num_turns]\n",
    "            _sequence_turn_ids = convert_turn_ids(_sequence_turn_ids, model_id=0, user_id=1)\n",
    "            _utterances, _speaker_ids = flatten_sequence(sequence_utterances=_sequence_utterances, sequence_turn_ids=_sequence_turn_ids)               \n",
    "            row_dict = dict()\n",
    "            row_dict[\"utterances\"] = _utterances\n",
    "            row_dict[\"speaker_ids\"] = _speaker_ids\n",
    "            \n",
    "            if _row_idx <= train_idx:\n",
    "                _train_rows.append(row_dict)\n",
    "            else:\n",
    "                _val_rows.append(row_dict)\n",
    "\n",
    "    train_output += _train_rows\n",
    "    val_output += _val_rows\n",
    "    print(\"\\t{} : (train: {}, val: {})\".format(dataset_name, len(_train_rows), len(_val_rows)), \"(empty_row_cnt: {})\".format(empty_row_cnt))\n",
    "    stat_dict[dataset_name][\"dialog_size\"] = (len(_train_rows), len(_val_rows))\n",
    "output = train_output + val_output\n",
    "print()\n",
    "    \n",
    "\n",
    "output_filename_template = \"feed_data_{idx}.json\"\n",
    "size_per_file = 100000\n",
    "\n",
    "print(\"# Write extracted dialogs...\")\n",
    "if shuffle: random.shuffle(output)\n",
    "if not os.path.exists(output_dir) or not os.path.isdir(output_dir): os.makedirs(output_dir)\n",
    "for idx in range(0, len(output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(output), (idx+1) * size_per_file)\n",
    "    _output = output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\t{}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "\n",
    "if not os.path.isdir(output_dir + \"train\"): os.mkdir(output_dir + \"train\")\n",
    "for idx in range(0, len(train_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(train_output), (idx+1) * size_per_file)\n",
    "    _output = train_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"train/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\ttrain {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "if not os.path.isdir(output_dir + \"val\"): os.mkdir(output_dir + \"val\")\n",
    "for idx in range(0, len(val_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(val_output), (idx+1) * size_per_file)\n",
    "    _output = val_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"val/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\tval {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "# write sample data\n",
    "if not os.path.isdir(output_dir + \"sample\"): os.mkdir(output_dir + \"sample\")\n",
    "sample_data_path = output_dir + \"sample/feed_data_sample.json\"\n",
    "with open(sample_data_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(output[:num_samples], fp)\n",
    "    \n",
    "# write DESC.md\n",
    "with open(output_dir + \"DESC.md\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    _str = \"## DESC\\n\" +\\\n",
    "        \"- Korean Dialog dataset for Language Model training\\n\" +\\\n",
    "        \"- total: {size}, avg_turns: {avg:.3f}\\n\".format(size=len(output), avg=np.mean([len(row[\"utterances\"]) for row in output])) +\\\n",
    "        \"- min turns: {mn_t}, max turns: {mx_t}\\n\".format(mn_t=min_num_turns, mx_t=max_num_turns) +\\\n",
    "        \"\\n\" +\\\n",
    "        \"## dataset statistics\\n\" +\\\n",
    "        \"### dataset_name: (avg_utterances, num_dialogues_before, num_dialogues_after)\\n\"\n",
    "    for dataset_name, stat in stat_dict.items():\n",
    "        _row_stat = \"\\t- {dataset_name}: ({avg_utterances}, {num_dialogues_before}, {num_dialogues_after})\\n\".format(dataset_name=dataset_name, avg_utterances=stat[\"dataset_size\"], num_dialogues_before=stat[\"avg_utterances\"], num_dialogues_after=sum(stat[\"dialog_size\"]))\n",
    "        _str += _row_stat\n",
    "    fp.write(_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><hr><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dialog finetuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5323439392779867"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host = \"3.143.59.250\"\n",
    "port = \"8000\"\n",
    "service = \"dialog-retriever\"\n",
    "\n",
    "method = \"infer-next-utterance\"\n",
    "body = {\n",
    "    \"utterances\": [],\n",
    "    \"speaker_ids\": [],\n",
    "    \"top_n\": 5,\n",
    "    \"max_retry\": 5\n",
    "}\n",
    "\n",
    "url = \"http://{host}:{port}/{service}/{method}\".format(host=host, port=port, service=service, method=method)\n",
    "\n",
    "from transformer.utils.common import get_nth_index\n",
    "def get_condition_from_remote_retriever(utterances, speaker_ids):\n",
    "    last_user_idx = get_nth_index(obj=speaker_ids, value=1, n=-1)\n",
    "    _utterances = utterances[0:last_user_idx+1]\n",
    "    _speaker_ids = speaker_ids[0:last_user_idx+1]\n",
    "    body[\"utterances\"] = _utterances\n",
    "    body[\"speaker_ids\"] = _speaker_ids\n",
    "    response = requests.post(url, json=body)\n",
    "    response_json = response.json()\n",
    "    \n",
    "    condition = None\n",
    "    if response_json[\"status\"]:\n",
    "        condition = [response_json[\"output\"][0][0]]\n",
    "    else:\n",
    "        print(\"{}: {}\".format(response_json[\"status\"], response_json[\"error_message\"]))\n",
    "        condition = utterances[last_user_idx+1:]\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.services.dialog_retriever.poly_encoder import DialogRetriever\n",
    "_model_dir = \"/home/mnt/guest1/model/poly_encoder/dialog_retriever/20210813/\"\n",
    "dr = DialogRetriever(temp_dir=\"./\")\n",
    "dr.load_model(model_dir=_model_dir)\n",
    "\n",
    "def get_condition_from_retriever(utterances, speaker_ids, top_n=5, max_retry=5):\n",
    "    condition = None\n",
    "    try:\n",
    "        output = dr.infer_next_utterance(utterances=utterances, speaker_ids=speaker_ids, top_n=top_n, max_retry=max_retry)\n",
    "        condition = output[0][0]\n",
    "    except:\n",
    "        print(\"length over\")\n",
    "        condition = utterances[last_user_idx+1:]\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition(utterances, speaker_ids, personas, entities):\n",
    "    user_persona = [persona for persona in personas if persona[\"id\"]==1][0]\n",
    "    is_user_condition = False\n",
    "    if \"name\" in entities and len(entities[\"name\"]) > 0:\n",
    "        last_entity_info = entities[\"name\"][-1]\n",
    "        entity_row_idx, entity_begin_idx, entity_end_idx, entity_span = last_entity_info\n",
    "        name_in_utterance = utterances[entity_row_idx][entity_begin_idx:entity_end_idx]\n",
    "        if entity_row_idx + 1 == len(utterances) and name_in_utterance in user_persona[\"name\"]: is_user_condition = True\n",
    "    \n",
    "    condition = None\n",
    "    if is_user_condition:\n",
    "        # 유저 정보를 이용하면, 컨디션으로 유저 정보를 넣어줘야지    \n",
    "        user_name = user_persona[\"name\"]\n",
    "        name_condition = {\"name\": user_name}\n",
    "        condition = [name_condition.__str__()]\n",
    "#         condition = [user_persona.__str__()]\n",
    "    else:\n",
    "        condition = None\n",
    "#         condition = get_condition_from_retriever(utterances=utterances, speaker_ids=speaker_ids)\n",
    "    return condition\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "def find_most_similar_persona(age, gender, default_name=\"당신\"):\n",
    "    target_df = persona_df\n",
    "    target_persona = {\"name\": default_name}\n",
    "    try:\n",
    "        target_df = target_df[target_df[\"gender\"]==gender]\n",
    "        age_diff = abs(target_df[\"age\"] - age)\n",
    "        target_df[\"age_diff\"] = list(age_diff)\n",
    "        target_age_diff = target_df.sort_values(\"age_diff\").values.tolist()[0][-1]\n",
    "        target_df = target_df.loc[(target_df[\"age_diff\"]==target_age_diff)]\n",
    "        target_persona = target_df.sample(1).to_dict(\"records\")[0]\n",
    "    except:\n",
    "        target_persona = target_df.sample(1).to_dict(\"records\")[0]\n",
    "    return target_persona\n",
    "\n",
    "def get_random_name():\n",
    "    random_name = np.random.choice(persona_df[\"name\"].unique().tolist())\n",
    "    return random_name\n",
    "\n",
    "# target_idx = 137\n",
    "# _utterances = output[target_idx][\"utterances\"][:-1]\n",
    "# _speaker_ids = output[target_idx][\"speaker_ids\"][:-1]\n",
    "# for _utterance, _speaker_id in zip(_utterances, _speaker_ids):\n",
    "#     print(\"{}: {}\".format(_speaker_id, _utterance))\n",
    "\n",
    "# body[\"utterances\"] = _utterances\n",
    "# body[\"speaker_ids\"] = _speaker_ids\n",
    "# response = requests.post(url, json=body)\n",
    "# response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load data from datasets...\n",
      "\t SelectStar : 2342 \t\tavg(utt_size): 31.0\n",
      "\t EmpatheticDialogues : 618 \t\tavg(utt_size): 18.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name_list = [\"SelectStar\", \"EmpatheticDialogues\"]\n",
    "# dataset_name_list = [\"SelectStar\"]\n",
    "output_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn/\"\n",
    "# output_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v1/\"\n",
    "\n",
    "# multi_turn: 기본 데이터, strictly-shuffled (리트리버 컨디션 & 유저 컨디션)\n",
    "# multi_turn_v1: 기본 데이터, lossely-shuffled (리트리버 컨디션 & 유저 컨디션)\n",
    "# multi_turn_v2: user_data condtion only, strictly-shuffled (name 정보만)\n",
    "# multi_turn_v3: user_data condtion only, lossely-shuffled (name 정보만)\n",
    "# multi_turn_v2: user_data condtion only, augmented * 5, strictly-shuffled (name 정보만)\n",
    "# multi_turn_v3: user_data condtion only, augmented * 5, lossely-shuffled (name 정보만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-5b837c5db94b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"condition\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"utterances\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "min_num_turns = 2\n",
    "max_num_turns = 6\n",
    "shuffle = True\n",
    "train_ratio = 0.85\n",
    "num_samples = 5000\n",
    "append_condition = True\n",
    "\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "print(\"# Load data from datasets...\")\n",
    "stat_dict = OrderedDict()\n",
    "data = OrderedDict()\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "    if \"multi_turn\" not in file_path[dataset_name][\"feed_data\"]: continue\n",
    "\n",
    "    data_path = file_path[dataset_name][\"feed_data\"][\"multi_turn\"].format(root_dir=root_dir, language=language)\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if not file_name.endswith(extension): continue\n",
    "        with open(data_path+file_name, \"r\", encoding=encoding) as fp:\n",
    "            _data = json.load(fp)\n",
    "\n",
    "            if dataset_name not in data: data[dataset_name] = []\n",
    "            data[dataset_name] += _data\n",
    "            _avg_utt_size = np.round(np.mean([len(r[\"utterances\"]) for r in _data]))\n",
    "            print(\"\\t\", dataset_name, \":\", len(_data), \"\\t\\tavg(utt_size):\", _avg_utt_size)\n",
    "            stat_dict[dataset_name] = dict()\n",
    "            stat_dict[dataset_name][\"dataset_size\"] = len(_data)\n",
    "            stat_dict[dataset_name][\"avg_utterances\"] = _avg_utt_size\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1470/2342 [00:00<00:00, 7363.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Extract utterances from data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2342/2342 [00:00<00:00, 7340.19it/s]\n",
      "100%|██████████| 618/618 [00:00<00:00, 13568.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSelectStar : (train: 179, val: 26) (empty_row_cnt: 1)\n",
      "\tEmpatheticDialogues : (train: 143, val: 72) (empty_row_cnt: 80)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Extract utterances from data...\")\n",
    "output = []\n",
    "train_output = []\n",
    "val_output = []\n",
    "\n",
    "u_output = []\n",
    "u_train_output = []\n",
    "u_val_output = []\n",
    "\n",
    "empty_row_cnt = 0\n",
    "for dataset_name, _data in data.items():\n",
    "    train_idx = int(len(_data) * train_ratio)\n",
    "    \n",
    "    _output = []\n",
    "    _train_output = []\n",
    "    _val_output = []\n",
    "    _u_output = []\n",
    "    _u_train_output = []\n",
    "    _u_val_output = []\n",
    "    for row_idx, row in enumerate(tqdm(_data, initial=0, total=len(_data), desc=dataset_name)): \n",
    "        if len([_utterance for _utterance in row[\"utterances\"] if _utterance!=\"\"]) < 1:\n",
    "            empty_row_cnt += 1\n",
    "            continue\n",
    "        \n",
    "        for begin_idx in range(0, (len(row[\"speaker_ids\"]) - max_num_turns)):    \n",
    "            for end_idx in range(begin_idx+1, len(row[\"speaker_ids\"])):\n",
    "                cur_num_turns = 0\n",
    "                _prev_speaker_id = row[\"speaker_ids\"][begin_idx]\n",
    "                for _speaker_id in row[\"speaker_ids\"][begin_idx+1:end_idx+1]:\n",
    "                    if _speaker_id != _prev_speaker_id:\n",
    "                        _prev_speaker_id = _speaker_id\n",
    "                        if _speaker_id == 1: cur_num_turns += 1\n",
    "\n",
    "                if begin_idx == 0 and cur_num_turns < 1: continue # 대화의 첫 발화이면 1턴 미만인 경우에 continue\n",
    "                if begin_idx != 0 and cur_num_turns < min_num_turns: continue # 대화의 첫 발화가 아니면 min_num_turns턴 미만인 경우에 continue\n",
    "                if cur_num_turns > max_num_turns: break # max_num_turns보다 크면 break\n",
    "                if row[\"speaker_ids\"][end_idx] != 0: continue # model_id로 발화가 끝나지 않는 경우 continue\n",
    "                if len(row[\"speaker_ids\"]) > end_idx+1 and row[\"speaker_ids\"][end_idx] == row[\"speaker_ids\"][end_idx+1]: continue # 다음 발화도 model_id의 것이면 continue\n",
    "\n",
    "                _utterances = row[\"utterances\"][begin_idx:end_idx+1]\n",
    "                _speaker_ids = row[\"speaker_ids\"][begin_idx:end_idx+1]\n",
    "                _labels = row[\"labels\"][begin_idx:end_idx+1]\n",
    "                _persona = row[\"persona\"]\n",
    "                if isinstance(_persona, dict): _persona = list(_persona.values())\n",
    "                _entities = dict()\n",
    "                for k,v in row[\"entities\"].items():\n",
    "                    _entities[k] = []\n",
    "                    for _v in v:\n",
    "                        _entitiy_row_idx, _entitiy_begin_idx, _entitiy_end_idx, _entitiy_span = _v\n",
    "                        entity_row_idx = _entitiy_row_idx - begin_idx\n",
    "                        if entity_row_idx < 0: continue\n",
    "                        if entity_row_idx > (end_idx - begin_idx): continue\n",
    "                        _v_row = [entity_row_idx, _entitiy_begin_idx, _entitiy_end_idx, _entitiy_span]\n",
    "                        _entities[k].append(_v_row)\n",
    "                \n",
    "                # utterance_length constraints: timestep보다 길면 탈락 (128 - num_special_tokens)\n",
    "                concat_utterances = \" \".join(_utterances)\n",
    "                if preprocessor.get_src_token_length(sentence=concat_utterances) >= 125: continue\n",
    "                    \n",
    "                row_dict = row.copy()\n",
    "                row_dict[\"utterances\"] = _utterances\n",
    "                row_dict[\"speaker_ids\"] = _speaker_ids\n",
    "                row_dict[\"labels\"] = _labels\n",
    "                row_dict[\"entities\"] = _entities\n",
    "                row_dict[\"persona\"] = _persona\n",
    "                _condition = get_condition(utterances=_utterances, speaker_ids=_speaker_ids, personas=_persona, entities=_entities)\n",
    "                row_dict[\"condition\"] = _condition\n",
    "\n",
    "                _output.append(row_dict)\n",
    "                u_reply = True if \"name\" in row_dict[\"entities\"] and len(row_dict[\"entities\"][\"name\"]) > 0 and row_dict[\"entities\"][\"name\"][-1][0] == len(row_dict[\"utterances\"])-1 else False\n",
    "                if u_reply: _u_output.append(row_dict)\n",
    "                if row_idx <= train_idx: \n",
    "                    _train_output.append(row_dict)\n",
    "                    if u_reply: _u_train_output.append(row_dict)\n",
    "                else: \n",
    "                    _val_output.append(row_dict)\n",
    "                    if u_reply: _u_val_output.append(row_dict)\n",
    "    \n",
    "    output += _output\n",
    "    train_output += _train_output\n",
    "    val_output += _val_output\n",
    "    u_output += _u_output\n",
    "    u_train_output += _u_train_output\n",
    "    u_val_output += _u_val_output\n",
    "    print(\"\\t{} : (train: {}, val: {})\".format(dataset_name, len(_train_output), len(_val_output)), \"(empty_row_cnt: {})\".format(empty_row_cnt))\n",
    "    stat_dict[dataset_name][\"dialog_size\"] = (len(_train_output), len(_val_output))\n",
    "    \n",
    "random.shuffle(train_output)\n",
    "random.shuffle(val_output)\n",
    "output = train_output + val_output\n",
    "\n",
    "random.shuffle(u_train_output)\n",
    "random.shuffle(u_val_output)\n",
    "u_output = u_train_output + u_val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Write extracted dialogs...\n",
      "\tfeed_data_00.json: 420\n",
      "\ttrain feed_data_00.json: 322\n",
      "\tval feed_data_00.json: 98\n"
     ]
    }
   ],
   "source": [
    "output_filename_template = \"feed_data_{idx}.json\"\n",
    "size_per_file = 100000\n",
    "\n",
    "print(\"# Write extracted dialogs...\")\n",
    "if shuffle: random.shuffle(output)\n",
    "if not os.path.exists(output_dir) or not os.path.isdir(output_dir): os.makedirs(output_dir)\n",
    "for idx in range(0, len(output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(output), (idx+1) * size_per_file)\n",
    "    _output = output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\t{}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "\n",
    "if not os.path.isdir(output_dir + \"train\"): os.mkdir(output_dir + \"train\")\n",
    "for idx in range(0, len(train_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(train_output), (idx+1) * size_per_file)\n",
    "    _output = train_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"train/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\ttrain {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "if not os.path.isdir(output_dir + \"val\"): os.mkdir(output_dir + \"val\")\n",
    "for idx in range(0, len(val_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(val_output), (idx+1) * size_per_file)\n",
    "    _output = val_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"val/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\tval {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "# write sample data\n",
    "if not os.path.isdir(output_dir + \"sample\"): os.mkdir(output_dir + \"sample\")\n",
    "sample_data_path = output_dir + \"sample/feed_data_sample.json\"\n",
    "with open(sample_data_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(output[:num_samples], fp)\n",
    "    \n",
    "# write DESC.md\n",
    "with open(output_dir + \"DESC.md\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    _str = \"## DESC\\n\" +\\\n",
    "        \"- Korean Dialog dataset for Language Model training\\n\" +\\\n",
    "        \"- total: {size}, avg_turns: {avg:.3f}\\n\".format(size=len(output), avg=np.mean([len(row[\"utterances\"]) for row in output])) +\\\n",
    "        \"- min turns: {mn_t}, max turns: {mx_t}\\n\".format(mn_t=min_num_turns, mx_t=max_num_turns) +\\\n",
    "        \"\\n\" +\\\n",
    "        \"## dataset statistics\\n\" +\\\n",
    "        \"### dataset_name: (avg_utterances, num_dialogues_before, num_dialogues_after)\\n\"\n",
    "    for dataset_name, stat in stat_dict.items():\n",
    "        _row_stat = \"\\t- {dataset_name}: ({avg_utterances}, {num_dialogues_before}, {num_dialogues_after})\\n\".format(dataset_name=dataset_name, avg_utterances=stat[\"dataset_size\"], num_dialogues_before=stat[\"avg_utterances\"], num_dialogues_after=sum(stat[\"dialog_size\"]))\n",
    "        _str += _row_stat\n",
    "    fp.write(_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_utterance = {\n",
    "    \"P\": \"즐거움\",\n",
    "    \"J\": \"기쁨\",\n",
    "    \"S\": \"슬픔\",\n",
    "    \"A\": \"분노\",\n",
    "    \"O\": \"보통\"\n",
    "}\n",
    "g_utterance = {\n",
    "    \"U\": \"유저정보\",\n",
    "    \"R\": \"페르소나\",\n",
    "    \"K\": \"외부지식\",\n",
    "    \"E\": \"공감형대화\",\n",
    "    \"N\": \"일반/기타\"\n",
    "}\n",
    "\n",
    "from collections import Counter\n",
    "labels = [label for row in output for label in row[\"labels\"]]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### augment user_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_augment = 5\n",
    "augmented_u_train_output = []\n",
    "augmented_u_val_output = []\n",
    "\n",
    "for row in u_train_output:\n",
    "    augmented_u_train_output.append(row)\n",
    "    \n",
    "    model_persona = [persona for persona in row[\"persona\"] if persona[\"id\"]==0][0]\n",
    "    user_persona = [persona for persona in row[\"persona\"] if persona[\"id\"]==1][0]\n",
    "    for i in range(0, num_augment):\n",
    "        _row = row.copy()\n",
    "        new_name = get_random_name()\n",
    "        new_persona = user_persona.copy()\n",
    "        new_persona[\"name\"] = new_name\n",
    "        _persona = [model_persona, new_persona]\n",
    "        _row[\"persona\"] = _persona\n",
    "\n",
    "        _utterances = _row[\"utterances\"].copy()\n",
    "        if \"name\" in _row[\"entities\"]:\n",
    "            name_to_change = new_name[1:] if len(new_name) == 3 else new_name\n",
    "            name_entity = []\n",
    "            for entity in _row[\"entities\"][\"name\"]:\n",
    "                _target_utterance = _utterances[entity[0]]\n",
    "                new_utterance = _target_utterance[:entity[1]] + name_to_change + _target_utterance[entity[2]:]\n",
    "                _utterances[entity[0]] = new_utterance\n",
    "                _entity = entity.copy()\n",
    "                _entity[2] = _entity[1] + len(name_to_change)\n",
    "                _entity[3] = len(name_to_change)\n",
    "                name_entity.append(_entity)\n",
    "            _row[\"entities\"][\"name\"] = name_entity\n",
    "            _row[\"utterances\"] = _utterances\n",
    "        _condition = get_condition(utterances=_row[\"utterances\"], speaker_ids=_row[\"speaker_ids\"], personas=_row[\"persona\"], entities=_row[\"entities\"])\n",
    "        if _condition is None:\n",
    "            raise AssertionError\n",
    "        else:\n",
    "            _row[\"condition\"] = _condition\n",
    "            \n",
    "        \n",
    "        augmented_u_train_output.append(_row)\n",
    "        \n",
    "for row in u_val_output:\n",
    "    augmented_u_val_output.append(row)\n",
    "    \n",
    "    model_persona = [persona for persona in row[\"persona\"] if persona[\"id\"]==0][0]\n",
    "    user_persona = [persona for persona in row[\"persona\"] if persona[\"id\"]==1][0]\n",
    "    for i in range(0, num_augment):\n",
    "        _row = row.copy()\n",
    "        new_name = get_random_name()\n",
    "        new_persona = user_persona.copy()\n",
    "        new_persona[\"name\"] = new_name\n",
    "        _persona = [model_persona, new_persona]\n",
    "        _row[\"persona\"] = _persona\n",
    "\n",
    "        _utterances = _row[\"utterances\"].copy()\n",
    "        if \"name\" in _row[\"entities\"]:\n",
    "            name_to_change = new_name[1:] if len(new_name) == 3 else new_name\n",
    "            name_entity = []\n",
    "            for entity in _row[\"entities\"][\"name\"]:\n",
    "                _target_utterance = _utterances[entity[0]]\n",
    "                new_utterance = _target_utterance[:entity[1]] + name_to_change + _target_utterance[entity[2]:]\n",
    "                _utterances[entity[0]] = new_utterance\n",
    "                _entity = entity.copy()\n",
    "                _entity[2] = _entity[1] + len(name_to_change)\n",
    "                _entity[3] = len(name_to_change)\n",
    "                name_entity.append(_entity)\n",
    "            _row[\"entities\"][\"name\"] = name_entity\n",
    "            _row[\"utterances\"] = _utterances\n",
    "        _condition = get_condition(utterances=_row[\"utterances\"], speaker_ids=_row[\"speaker_ids\"], personas=_row[\"persona\"], entities=_row[\"entities\"])\n",
    "        if _condition is None:\n",
    "            raise AssertionError\n",
    "        else:\n",
    "            _row[\"condition\"] = _condition\n",
    "        \n",
    "        augmented_u_val_output.append(_row)\n",
    "        \n",
    "random.shuffle(augmented_u_train_output)\n",
    "random.shuffle(augmented_u_val_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><hr><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_utterance = {\n",
    "    \"P\": \"즐거움\",\n",
    "    \"J\": \"기쁨\",\n",
    "    \"S\": \"슬픔\",\n",
    "    \"A\": \"분노\",\n",
    "    \"O\": \"보통\"\n",
    "}\n",
    "g_utterance = {\n",
    "    \"U\": \"유저정보\",\n",
    "    \"R\": \"페르소나\",\n",
    "    \"K\": \"외부지식\",\n",
    "    \"E\": \"공감형대화\",\n",
    "    \"N\": \"일반/기타\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 2342\n",
      "error nums: 0\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"SelectStar\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(raw_dir):\n",
    "    if not filename.endswith(extension): continue\n",
    "    with open(raw_dir+filename, \"r\", encoding=encoding) as fp:\n",
    "        __data = json.load(fp)\n",
    "        \n",
    "        _data = []\n",
    "        for row in __data:\n",
    "            if \"speakers_ids\" in row:\n",
    "                row[\"speaker_ids\"] = row.pop(\"speakers_ids\")\n",
    "            _data.append(row)\n",
    "        data += _data\n",
    "print(\"data size:\", len(data))\n",
    "\n",
    "error_indice = []\n",
    "for row_idx, row in enumerate(data):\n",
    "    error_list = []\n",
    "    if len(row[\"persona\"])!=2: \n",
    "        error_list.append(\"persona length: {}\".format(len(row[\"persona\"])))\n",
    "    if len(row[\"utterances\"])!=len(row[\"speaker_ids\"]) or len(row[\"utterances\"])!=len(row[\"labels\"]) or len(row[\"speaker_ids\"])!=len(row[\"labels\"]): \n",
    "        error_list.append(\"length difference: {}/{}/{}\".format(len(row[\"utterances\"]),len(row[\"speaker_ids\"]),len(row[\"labels\"])))\n",
    "\n",
    "    for speaker_id, label in zip(row[\"speaker_ids\"], row[\"labels\"]):\n",
    "        if speaker_id == 0 and label not in g_utterance:\n",
    "            error_list.append(\"wrong label for g: '{}'\".format(label))\n",
    "        if speaker_id == 1 and label not in u_utterance:\n",
    "            error_list.append(\"wrong label for u: '{}'\".format(label))\n",
    "\n",
    "    if len(error_list) > 0:\n",
    "        error_indice.append((row_idx, error_list))\n",
    "        print(\"error: {}\".format(row_idx))\n",
    "        \n",
    "string_output = \"\"\n",
    "for row in error_indice:\n",
    "    _str = \"{}:\\n\".format(row[0])\n",
    "    for error in row[1]:\n",
    "        _str += \"\\t{}\\n\".format(error)\n",
    "    string_output += _str\n",
    "    \n",
    "print(\"error nums:\", len(error_indice))\n",
    "if len(error_indice) <= 0:\n",
    "    random.shuffle(output)\n",
    "    with open(root_dir + \"/{language}/multi_turn/feed_data_0.json\".format(language=language), \"w\", encoding=encoding) as fp:\n",
    "        json.dump(data, fp)\n",
    "else:\n",
    "    with open(\"./error_contents.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(string_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_SSGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AIHUB_SSGI\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "final_output = []\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir+\"dialog\") if file_name.endswith(\".xlsx\")]\n",
    "for file_name in file_name_list:\n",
    "    df = pd.read_excel(raw_dir+\"dialog/\"+file_name, engine=\"openpyxl\")\n",
    "    df = df.fillna(\"\")\n",
    "    \n",
    "    speaker_info = dict()\n",
    "    domain = None\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    categories = []\n",
    "    tasks = []\n",
    "    subtasks = []\n",
    "\n",
    "    idx = 0\n",
    "    output = []\n",
    "    for row in df[[\"SPEAKER\", \"SENTENCE\", \"DOMAIN\", \"CATEGORY\", \"MAIN\", \"SUB\", \"SPEAKERID\", \"SENTENCEID\"]].values.tolist():\n",
    "        speaker, sentence, _domain, category, main, sub, speaker_id, sentence_id = row\n",
    "\n",
    "        if int(sentence_id) == 1 and len(utterances) > 0:\n",
    "            # append\n",
    "            output_row = OrderedDict()\n",
    "            output_row[\"idx\"] = idx\n",
    "            output_row[\"speakers\"] = dict(OrderedDict(sorted(speaker_info.items())))\n",
    "            output_row[\"domain\"] = domain\n",
    "            output_row[\"utterances\"] = utterances\n",
    "            output_row[\"speaker_ids\"] = speaker_ids\n",
    "            output_row[\"category\"] = trim_obj(categories)\n",
    "            output_row[\"task\"] = trim_obj(tasks)\n",
    "            output_row[\"subtask\"] = trim_obj(subtasks)\n",
    "            output_row = dict(output_row)\n",
    "            output.append(output_row)\n",
    "\n",
    "            # reset\n",
    "            idx += 1\n",
    "            speaker_info = dict()\n",
    "            domain = None\n",
    "            utterances = []\n",
    "            speaker_ids = []\n",
    "            categories = []\n",
    "            tasks = []\n",
    "            subtasks = []\n",
    "\n",
    "        speaker_info[speaker_id] = speaker\n",
    "        domain = _domain\n",
    "        utterances.append(sentence)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        categories.append(category)\n",
    "        tasks.append(main)\n",
    "        subtasks.append(sub)\n",
    "\n",
    "    # append\n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"speakers\"] = speaker_info\n",
    "    output_row[\"domain\"] = domain\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"category\"] = categories\n",
    "    output_row[\"task\"] = tasks\n",
    "    output_row[\"subtask\"] = subtasks\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)\n",
    "\n",
    "    final_output += output\n",
    "    print(\"output_size:\", len(output), \"final_output size:\", len(final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AIHUB_Twitter\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "df = pd.read_excel(raw_dir+\"트위터_대화시나리오DB_2000Set.xlsx\", engine=\"openpyxl\")\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "output = []\n",
    "for row in df.values.tolist():\n",
    "    utterances = []\n",
    "    for utterance in row:\n",
    "        if utterance == \"\": break\n",
    "        utterances.append(utterance)\n",
    "    speaker_ids = [i%2 for i in range(0, len(utterances))]\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_EMOTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"xlsx\"\n",
    "\n",
    "dataset_name = \"AIHUB_EMOTION\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file_name in os.listdir(raw_dir):\n",
    "    if not file_name.endswith(extension): continue\n",
    "    _df = pd.read_excel(raw_dir+file_name)\n",
    "    df = pd.concat([df, _df], axis=0)\n",
    "df = df.fillna(\"\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "output = []\n",
    "domain = \"emotional_dialog\"\n",
    "for row_idx, row in enumerate(df.values.tolist()):\n",
    "    _, _, age_range, gender, category, healt_status, task, subtask, u1, s1, u2, s2, u3, s3 = row\n",
    "    if gender == \"여성\": gender = \"F\"\n",
    "    elif gender == \"남성\": gender = \"M\"\n",
    "    else: gender = \"O\"\n",
    "    speakers = {\"0\":\"시스템\", \"1\":\"사용자\"}\n",
    "    utterances = [u1, s1, u2, s2, u3, s3]\n",
    "    utterances = [utterance for utterance in utterances if utterance!=\"\"]\n",
    "    speaker_ids = [(i+1)%2 for i in range(0, len(utterances))]\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"speakers\"] = speakers\n",
    "    output_row[\"domain\"] = domain\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"category\"] = category\n",
    "    output_row[\"task\"] = task\n",
    "    output_row[\"subtask\"] = subtask\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_SNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "\n",
    "dataset_name = \"AIHUB_SNS\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "data = []\n",
    "for file_name in os.listdir(raw_dir):\n",
    "    if not file_name.endswith(extension): continue\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        _data = json.load(fp)\n",
    "        data += _data[\"data\"]\n",
    "    break\n",
    "\n",
    "output = []\n",
    "for row_idx, row in enumerate(data):\n",
    "    domain = row[\"header\"][\"dialogueInfo\"][\"type\"]\n",
    "    category = row[\"header\"][\"dialogueInfo\"][\"topic\"]\n",
    "    speakers = OrderedDict()\n",
    "    speaker_mapping = dict()\n",
    "    for speaker_id, speaker in enumerate(row[\"header\"][\"participantsInfo\"]):\n",
    "        speaker_mapping[speaker[\"participantID\"]] = speaker_id\n",
    "        speakers[speaker_id] = OrderedDict()\n",
    "        speakers[speaker_id][\"id\"] = speaker[\"participantID\"]\n",
    "        speakers[speaker_id][\"sex\"] = speaker[\"gender\"]\n",
    "        speakers[speaker_id][\"age\"] = speaker[\"age\"]\n",
    "        speakers[speaker_id][\"residence\"] = speaker[\"residentialProvince\"]\n",
    "        speakers[speaker_id] = dict(speakers[speaker_id])\n",
    "    speakers = dict(speakers)\n",
    "    summary = row[\"body\"][\"summary\"]\n",
    "    \n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    datetime_list = []\n",
    "    for _row in row[\"body\"][\"dialogue\"]:\n",
    "        utterance = _row[\"utterance\"]\n",
    "        utterances.append(utterance)\n",
    "        speaker_id = speaker_mapping[_row[\"participantID\"]]\n",
    "        speaker_ids.append(speaker_id)\n",
    "        _datetime = _row[\"date\"] + \" \" + _row[\"time\"]\n",
    "        datetime_list.append(_datetime)\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = row_idx\n",
    "    output_row[\"speakers\"] = speakers\n",
    "    output_row[\"domain\"] = domain\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"datetime\"] = datetime_list\n",
    "    output_row[\"category\"] = category\n",
    "    output_row[\"summary\"] = summary\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLUE_DST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"KLUE_DST\" # 국립국어원\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir) if file_name.endswith(\".json\") and file_name.startswith(\"wos\")]\n",
    "\n",
    "output = []\n",
    "kor_regex = \"[ㄱ-ㅣ가-힣]\"\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    for dialog_idx, dialog_row in enumerate(data):\n",
    "        dialog = dialog_row[\"dialogue\"]\n",
    "\n",
    "        output_row = OrderedDict()\n",
    "        utterances = []\n",
    "        speaker_ids = []\n",
    "        category = []\n",
    "        task = []\n",
    "        subtask = []\n",
    "        _speaker_info = dict()\n",
    "        for row in dialog:\n",
    "            speaker = row[\"role\"]\n",
    "            if speaker not in _speaker_info:\n",
    "                _speaker_info[speaker] = len(_speaker_info)\n",
    "            utterance = row[\"text\"]\n",
    "            utterances.append(utterance)\n",
    "            speaker_id = _speaker_info[speaker]\n",
    "            speaker_ids.append(speaker_id)\n",
    "\n",
    "            if \"state\" not in row: continue\n",
    "            state = row[\"state\"]\n",
    "            for _state in state:\n",
    "                _state = _state.split(\"-\")\n",
    "                if len(_state) > 0: \n",
    "                    _category = _state[0]\n",
    "                    category.append(_category)\n",
    "                if len(_state) > 1:\n",
    "                    _task = _state[1]\n",
    "                    task.append(_task)\n",
    "                if len(_state) > 2:\n",
    "                    _subtask = _state[2]\n",
    "                    if re.search(kor_regex, _subtask) is not None:\n",
    "                        subtask.append(_subtask)\n",
    "            category = trim_obj(category)\n",
    "            task = trim_obj(task)\n",
    "            subtask = trim_obj(subtask)\n",
    "\n",
    "        speaker_info = {v:k for k,v in _speaker_info.items()}\n",
    "\n",
    "        output_row[\"idx\"] = dialog_idx\n",
    "        output_row[\"speakers\"] = speaker_info\n",
    "        output_row[\"utterances\"] = utterances\n",
    "        output_row[\"speaker_ids\"] = speaker_ids\n",
    "        output_row[\"category\"] = category\n",
    "        output_row[\"task\"] = task\n",
    "        output_row[\"subtask\"] = subtask\n",
    "        output_row = dict(output_row)\n",
    "        output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIKL_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"NIKL_2020\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir) if file_name.endswith(\".json\")]\n",
    "output = []\n",
    "for idx, file_name in enumerate(file_name_list):\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # date\n",
    "    # date = datetime.strptime(data[\"document\"][0][\"metadata\"][\"date\"], \"%Y%M%d\").strftime(\"%Y-%M-%d\")\n",
    "    _date = data[\"document\"][0][\"metadata\"][\"date\"]\n",
    "    date = \"-\".join([_date[0:4], _date[4:6], _date[6:8]])\n",
    "    # tasks\n",
    "    tasks = data[\"document\"][0][\"metadata\"][\"topic\"]\n",
    "    tasks = tasks.split(\">\")[-1]\n",
    "    tasks = [task.strip() for task in tasks.split(\",\")]\n",
    "    tasks = trim_obj(tasks)\n",
    "    # speaker_info\n",
    "    spkid2id = {speaker[\"id\"]:_idx for _idx, speaker in enumerate(data[\"document\"][0][\"metadata\"][\"speaker\"])}\n",
    "    speaker_info = {spkid2id[speaker[\"id\"]]:speaker for speaker in data[\"document\"][0][\"metadata\"][\"speaker\"]}\n",
    "    speaker_info = dict(OrderedDict(sorted(speaker_info.items())))\n",
    "    # utterances & speaker_ids\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    elements = data[\"document\"][0][\"utterance\"]\n",
    "    prev_utterance = elements[0][\"form\"] # 철자 전사\n",
    "    # utterance = elements[0][\"original_form\"] # 발음 전사\n",
    "    prev_speaker_id = elements[0][\"speaker_id\"]\n",
    "    for element_idx in range(1, len(elements)):\n",
    "        cur_utterance = elements[element_idx][\"form\"] # 철자 전사\n",
    "        # cur_utterance = elements[element_idx][\"original_form\"] # 발음 전사\n",
    "        cur_speaker_id = elements[element_idx][\"speaker_id\"]\n",
    "\n",
    "        if cur_speaker_id == prev_speaker_id:\n",
    "            if prev_utterance.endswith(\".\"): \n",
    "                utterances.append(prev_utterance)\n",
    "                speaker_ids.append(prev_speaker_id)\n",
    "                prev_utterance = cur_utterance\n",
    "            else:\n",
    "                prev_utterance = prev_utterance + cur_utterance\n",
    "        else:\n",
    "            for _prev_utterance in prev_utterance.split(\". \"):\n",
    "                if _prev_utterance.strip() == \"\": continue\n",
    "                if prev_utterance.strip().endswith(\".\") and not _prev_utterance.strip().endswith(\".\"): \n",
    "                    _prev_utterance = _prev_utterance + \".\"\n",
    "                utterances.append(_prev_utterance)\n",
    "                speaker_ids.append(prev_speaker_id)\n",
    "            # reset\n",
    "            prev_utterance = cur_utterance\n",
    "            prev_speaker_id = cur_speaker_id\n",
    "    utterances.append(prev_utterance)\n",
    "    speaker_ids.append(prev_speaker_id)\n",
    "    speaker_ids = [spkid2id[speaker_id] for speaker_id in speaker_ids]\n",
    "\n",
    "\n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"document_id\"] = data[\"document\"][0][\"id\"]\n",
    "    output_row[\"date\"] = date\n",
    "    output_row[\"task\"] = tasks\n",
    "    output_row[\"speaker_info\"] = speaker_info\n",
    "    output_row[\"setting\"] = data[\"document\"][0][\"metadata\"][\"setting\"]\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIKL_Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_row(idx, soup):\n",
    "    output_row = OrderedDict()\n",
    "    speaker_info = dict()\n",
    "    _speaker_id_mapping = dict()\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "\n",
    "    header = soup.find(\"body\").find(\"teiheader\")\n",
    "    _file_desc = header.find(\"filedesc\")\n",
    "    _profile_desc = header.find(\"profiledesc\")\n",
    "    speakers = _profile_desc.findAll(\"person\")\n",
    "    for speaker_id, speaker in enumerate(speakers):\n",
    "        _speaker_info = OrderedDict(speaker.attrs)\n",
    "        if len(splited) > 0:\n",
    "            _speaker_info[\"job\"] = splited[0].strip()\n",
    "        if len(splited) > 1:\n",
    "            _speaker_info[\"residence\"] = splited[1].strip()\n",
    "        speaker_info[speaker_id] = dict(_speaker_info)\n",
    "        _speaker_id_mapping[_speaker_info[\"id\"]] = speaker_id\n",
    "\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "\n",
    "    prev_speaker_id = -1\n",
    "    prev_utterance = \"\"\n",
    "    _utterances = soup.findAll(\"u\")\n",
    "    for _utterance in _utterances:\n",
    "        _speaker_id = _utterance.attrs[\"who\"]\n",
    "        if _speaker_id not in _speaker_id_mapping: continue\n",
    "        speaker_id = _speaker_id_mapping[_speaker_id]\n",
    "        # utterance = [u.text.strip() for u in _utterance.findAll(\"s\")]\n",
    "        utterance = [child.strip() for s in _utterance.findAll(\"s\") for child in s.children if isinstance(child, str)]\n",
    "        utterance = \" \".join(utterance)\n",
    "        utterance = utterance.replace(\"::\", \"\")\n",
    "        if utterance == \"\": continue\n",
    "\n",
    "        if speaker_id == prev_speaker_id:\n",
    "            prev_utterance = prev_utterance + \" \" + utterance\n",
    "        else:\n",
    "            if prev_speaker_id != -1:\n",
    "                utterances.append(prev_utterance)\n",
    "                speaker_ids.append(prev_speaker_id)\n",
    "            prev_utterance = utterance\n",
    "            prev_speaker_id = speaker_id\n",
    "\n",
    "    if speaker_id == prev_speaker_id:\n",
    "        prev_utterance = prev_utterance + utterance\n",
    "    utterances.append(prev_utterance)\n",
    "    speaker_ids.append(prev_speaker_id)\n",
    "    \n",
    "    assert len(utterances) == len(speaker_ids), \"{l1} vs {l2}\".format(l1=len(utterances), l2=len(speaker_ids))\n",
    "\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"speakers\"] = speaker_info\n",
    "    output_row[\"title\"] = _file_desc.find(\"title\").text\n",
    "    output_row[\"domain\"] = _profile_desc.find(\"settingdesc\").text\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"project_name\"] = header.find(\"projectdesc\").text\n",
    "    output_row[\"distributor\"] = _file_desc.find(\"distributor\").text\n",
    "    extent = _file_desc.find(\"extent\").text\n",
    "    output_row[\"num_eojeols\"] = int(re.sub(\"[^0-9]\", \"\", extent))\n",
    "    output_row = dict(output_row)\n",
    "    return output_row\n",
    "\n",
    "file_path_list = []\n",
    "for sub_dir in os.listdir(raw_dir):\n",
    "    file_path = raw_dir+sub_dir+\"/원시/\"\n",
    "    if not os.path.isdir(file_path): continue\n",
    "    file_name_list = os.listdir(file_path)\n",
    "    if len(file_name_list) != 1: continue\n",
    "    file_name = file_name_list[0]\n",
    "    file_path_list.append(file_path+file_name)\n",
    "    \n",
    "output = []\n",
    "for idx, file_path in enumerate(file_path_list):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        contents = f.read()\n",
    "    contents = contents.decode(\"utf-16\")\n",
    "    contents = re.sub(\"\\r\\n\", \"\", contents)\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    output_row = get_output_row(idx=idx, soup=soup)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"OpenSubtitles\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "pickle_path = file_path[dataset_name][\"pickle\"].format(root_dir=root_dir, language=language)\n",
    "data = None\n",
    "with open(pickle_path, \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "output = []\n",
    "kor_regex = \"[ㄱ-ㅣ가-힣]\"\n",
    "\n",
    "_data = data[\"train\"] + data[\"test\"]\n",
    "idx = 0\n",
    "for row in _data:\n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"file_id\"] = row.pop(\"file_id\")\n",
    "    context = row.pop(\"context\")\n",
    "    response = row.pop(\"response\")\n",
    "    \n",
    "    _utterances = [row[k] for k in sorted(row.keys(), reverse=True)]\n",
    "    _utterances.append(context)\n",
    "    _utterances.append(response)\n",
    "\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    _idx = 0\n",
    "    \n",
    "    not_korean = True\n",
    "    for utterance in _utterances:\n",
    "        speaker_id = -1 * ((_idx+1) % 2)\n",
    "        _idx += 1\n",
    "        utterance = utterance.replace(\"'\", \"\")\n",
    "        utterance = utterance.replace('\"', '')\n",
    "        utterance = re.sub(\"^.* :\", \"\", utterance).strip()\n",
    "        utterance = utterance.replace(\"nbsp;\", \"\").strip()\n",
    "        if utterance==\"\": continue\n",
    "        utterances.append(utterance)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        if re.search(kor_regex, utterance) is not None: not_korean = False\n",
    "    \n",
    "    if not_korean: continue\n",
    "        \n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmpatheticDialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 2342\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"SelectStar\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(raw_dir):\n",
    "    if not filename.endswith(extension): continue\n",
    "    with open(raw_dir+filename, \"r\", encoding=encoding) as fp:\n",
    "        __data = json.load(fp)\n",
    "        \n",
    "        _data = []\n",
    "        for row in __data:\n",
    "            if \"speakers_ids\" in row:\n",
    "                row[\"speaker_ids\"] = row.pop(\"speakers_ids\")\n",
    "            _data.append(row)\n",
    "        data += _data\n",
    "print(\"data size:\", len(data))\n",
    "total_personas = [persona for row in data for persona in row[\"persona\"].values()]\n",
    "persona_df = pd.DataFrame(total_personas)\n",
    "persona_df = persona_df.fillna(\"\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "def find_most_similar_persona(age, gender, default_name=\"당신\"):\n",
    "    target_df = persona_df\n",
    "    target_persona = {\"name\": default_name}\n",
    "    try:\n",
    "        target_df = target_df[target_df[\"gender\"]==gender]\n",
    "        age_diff = abs(target_df[\"age\"] - age)\n",
    "        target_df[\"age_diff\"] = list(age_diff)\n",
    "        target_age_diff = target_df.sort_values(\"age_diff\").values.tolist()[0][-1]\n",
    "        target_df = target_df.loc[(target_df[\"age_diff\"]==target_age_diff)]\n",
    "        target_persona = target_df.sample(1).to_dict(\"records\")[0]\n",
    "    except:\n",
    "        target_persona = target_df.sample(1).to_dict(\"records\")[0]\n",
    "    return target_persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"EmpatheticDialogues\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"xlsx\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "filename_list = os.listdir(raw_dir)\n",
    "df = pd.DataFrame()\n",
    "for filename in filename_list:\n",
    "    if not filename.endswith(extension): continue\n",
    "    _df = pd.read_excel(raw_dir + filename)\n",
    "    df = pd.concat([df, _df], axis=0)\n",
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make preprocessed raw_data\n",
    "data = []\n",
    "\n",
    "name_entity_pattern = \"\\$\\$[ㄱ-ㅣ가-힣]+\\$\\$\"\n",
    "prev_row_idx = -1\n",
    "utterances = []\n",
    "speaker_ids = []\n",
    "personas = []\n",
    "entities = dict()\n",
    "turn_idx = 0\n",
    "\n",
    "for _idx, row in enumerate(df.values.tolist()):    \n",
    "    row_idx, persona_idx, eng, kor_before, kor_after, gender, age, married, job = row\n",
    "    \n",
    "    if _idx == 0: \n",
    "        prev_row_idx = row_idx\n",
    "    if _idx > 0 and row_idx != \"\" and prev_row_idx != int(row_idx):\n",
    "        # append dialog\n",
    "        output_row = OrderedDict()\n",
    "        output_row[\"idx\"] = int(prev_row_idx)\n",
    "        output_row[\"utterances\"] = utterances\n",
    "        output_row[\"speaker_ids\"] = speaker_ids\n",
    "        output_row[\"persona\"] = personas\n",
    "        output_row[\"entities\"] = entities\n",
    "        output_row = dict(output_row)\n",
    "        data.append(output_row)\n",
    "        # reset\n",
    "        prev_row_idx = row_idx\n",
    "        utterances = []\n",
    "        speaker_ids = []\n",
    "        personas = []\n",
    "        entities = dict()\n",
    "        turn_idx = 0\n",
    "        \n",
    "    _utterances = clean(str(kor_after))\n",
    "    speaker_id = (turn_idx+1) % 2\n",
    "    for utterance in _utterances.split(\". \"):\n",
    "        utterances.append(utterance)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        \n",
    "    if gender!=\"\":\n",
    "        persona = OrderedDict()\n",
    "        persona[\"id\"] = speaker_id\n",
    "        persona[\"persona_idx\"] = int(persona_idx)\n",
    "        persona[\"gender\"] = gender\n",
    "        if age != \"\": \n",
    "            persona[\"age\"] = int(age)\n",
    "        if married != \"\": \n",
    "            persona[\"married\"] = int(married)\n",
    "        if job != \"\": \n",
    "            persona[\"job\"] = int(job)\n",
    "        persona = dict(persona)\n",
    "        personas.append(persona)\n",
    "    turn_idx += 1\n",
    "    \n",
    "# append dialog\n",
    "output_row = OrderedDict()\n",
    "output_row[\"idx\"] = int(prev_row_idx)\n",
    "output_row[\"utterances\"] = utterances\n",
    "output_row[\"speaker_ids\"] = speaker_ids\n",
    "output_row[\"persona\"] = personas\n",
    "output_row[\"entities\"] = entities\n",
    "output_row = dict(output_row)\n",
    "data.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 채워넣기\n",
    "_data = data.copy()\n",
    "data = []\n",
    "for row in _data: \n",
    "    new_personas = []\n",
    "    for persona in row[\"persona\"]:\n",
    "        new_persona = find_most_similar_persona(age=persona[\"age\"], gender=persona[\"gender\"])\n",
    "        persona[\"name\"] = new_persona[\"name\"]\n",
    "        new_personas.append(persona)\n",
    "    row[\"persona\"] = new_personas\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 치환 & entities 정보 수집\n",
    "name_entity_pattern = \"\\$\\$[ㄱ-ㅣ가-힣]+\\$\\$\"\n",
    "\n",
    "_data = data.copy()\n",
    "data = []\n",
    "\n",
    "for _row in _data:\n",
    "    row = _row.copy()\n",
    "    \n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    labels = []\n",
    "    entities = dict()\n",
    "    for utterance_idx, (_utterance, _speaker_id) in enumerate(zip(row[\"utterances\"], row[\"speaker_ids\"])):\n",
    "        label = \"\" if _speaker_id == 1 else \"N\"\n",
    "        \n",
    "        while True:\n",
    "            name_entity_search = re.search(name_entity_pattern, _utterance)\n",
    "            if name_entity_search is not None:\n",
    "                cur_persona = [persona for persona in row[\"persona\"] if persona[\"id\"]==(_speaker_id+1)%2][0]\n",
    "                entity_start_idx = name_entity_search.start()\n",
    "                entity_end_idx = name_entity_search.end()\n",
    "\n",
    "                name = cur_persona[\"name\"]\n",
    "                if len(name) == 3: name = name[1:]\n",
    "                _utterance = _utterance[:entity_start_idx] + name + _utterance[entity_end_idx:]\n",
    "                entity_span = len(name)\n",
    "                entity_end_idx = entity_start_idx + entity_span\n",
    "\n",
    "                entity = [utterance_idx, entity_start_idx, entity_end_idx, entity_span]\n",
    "                if \"name\" not in entities: entities[\"name\"] = []\n",
    "                entities[\"name\"].append(entity)\n",
    "                if _speaker_id == 0: label = \"U\"\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        utterances.append(_utterance)\n",
    "        speaker_ids.append(_speaker_id)\n",
    "        labels.append(label)\n",
    "\n",
    "    row[\"utterances\"] = utterances\n",
    "    row[\"speaker_ids\"] = speaker_ids\n",
    "    row[\"labels\"] = labels\n",
    "    row[\"entities\"] = entities\n",
    "    \n",
    "    data.append(row)\n",
    "\n",
    "# with open(raw_dir + \"empathetic_dialogues.pickle\", \"wb\") as fp:\n",
    "#     pickle.dump(data, fp)\n",
    "# with open(root_dir + \"/{language}/multi_turn/feed_data_0.json\".format(language=language), \"w\", encoding=encoding) as fp:\n",
    "#     json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"EmpatheticDialogues\"\n",
    "language = \"eng\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "_data = None\n",
    "with open(raw_dir+\"empathetic_dialogues.pickle\", \"rb\") as fp:\n",
    "    _data = pickle.load(fp)\n",
    "\n",
    "output = []\n",
    "row_idx = 0\n",
    "for k,v in _data.items():\n",
    "    print(\"v size:\", len(v))\n",
    "    print(\"v conv size:\", len(pd.DataFrame(v)[\"conv_id\"].unique()))\n",
    "    rows = []\n",
    "\n",
    "    prev_conv_id = None\n",
    "    output_row = OrderedDict()\n",
    "    for row in v:\n",
    "        cur_conv_id = row[\"conv_id\"]\n",
    "        if prev_conv_id is None:\n",
    "            output_row[\"idx\"] = row_idx\n",
    "            output_row[\"context\"] = row[\"context\"]\n",
    "            output_row[\"prompt\"] = row[\"prompt\"]\n",
    "            output_row[\"utterances\"] = []\n",
    "            output_row[\"speaker_ids\"] = []\n",
    "            output_row[\"selfeval\"] = row[\"selfeval\"]\n",
    "            output_row[\"tags\"] = []\n",
    "            prev_conv_id = cur_conv_id\n",
    "            continue\n",
    "\n",
    "        if cur_conv_id != prev_conv_id:\n",
    "            tags = sorted(set(output_row[\"tags\"]))\n",
    "            output_row[\"tags\"] = tags\n",
    "            prompt = output_row.pop(\"prompt\")\n",
    "            output_row[\"utterances\"].insert(0, prompt)\n",
    "            if len(output_row[\"speaker_ids\"]) == 0:\n",
    "                previous_speaker_id = 0\n",
    "            elif len(output_row[\"speaker_ids\"]) > 1:\n",
    "                previous_speaker_id = output_row[\"speaker_ids\"][1]\n",
    "            else:\n",
    "                if output_row[\"speaker_ids\"][0] == 0:\n",
    "                    previous_speaker_id = 1\n",
    "                else:\n",
    "                    previous_speaker_id = 0\n",
    "            output_row[\"speaker_ids\"].insert(0, previous_speaker_id)\n",
    "            output_row = dict(output_row)\n",
    "            rows.append(output_row)\n",
    "            prev_conv_id = cur_conv_id\n",
    "            row_idx += 1\n",
    "\n",
    "            output_row = OrderedDict()\n",
    "            output_row[\"idx\"] = row_idx\n",
    "            output_row[\"context\"] = row[\"context\"]\n",
    "            output_row[\"prompt\"] = row[\"prompt\"]\n",
    "            output_row[\"utterances\"] = []\n",
    "            output_row[\"speaker_ids\"] = []\n",
    "            output_row[\"selfeval\"] = row[\"selfeval\"]\n",
    "            output_row[\"tags\"] = []\n",
    "        else:\n",
    "            output_row[\"utterances\"].append(row[\"utterance\"])\n",
    "            output_row[\"speaker_ids\"].append(int(row[\"speaker_idx\"]))\n",
    "            output_row[\"tags\"].append(row[\"tags\"])\n",
    "\n",
    "    tags = sorted(set(output_row[\"tags\"]))\n",
    "    output_row[\"tags\"] = tags\n",
    "    prompt = output_row.pop(\"prompt\")\n",
    "    output_row[\"utterances\"].insert(0, prompt)\n",
    "    if len(output_row[\"speaker_ids\"]) > 1:\n",
    "        previous_speaker_id = output_row[\"speaker_ids\"][1]\n",
    "    else:\n",
    "        if output_row[\"speaker_ids\"][0] == 0:\n",
    "            previous_speaker_id = 1\n",
    "        else:\n",
    "            previous_speaker_id = 0\n",
    "    output_row[\"speaker_ids\"].insert(0, previous_speaker_id)\n",
    "    output_row = dict(output_row)\n",
    "    rows.append(output_row)\n",
    "    prev_conv_id = cur_conv_id\n",
    "    row_idx += 1\n",
    "    \n",
    "    print(\"rows size:\", len(rows), \"\\n\")\n",
    "    output += rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KaggleConversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"KaggleConversation\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "titles = pd.read_csv(raw_dir + \"conversation_titles.csv\")\n",
    "data_df = pd.read_csv(raw_dir + \"conversations.csv\")\n",
    "\n",
    "output = []\n",
    "for row_idx, _date in enumerate(titles[\"date\"].unique().tolist()):\n",
    "    title = titles[titles[\"date\"]==_date][\"kor_title\"].tolist()[0]\n",
    "    translated_title = titles[titles[\"date\"]==_date][\"eng_title\"].tolist()[0]\n",
    "    utterances = data_df[data_df[\"date\"]==_date][\"kor_sent\"].tolist()\n",
    "    translated_utterances = data_df[data_df[\"date\"]==_date][\"eng_sent\"].tolist()\n",
    "    speaker_ids = [(i+1)%2 for i in range(0, len(utterances))]\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = row_idx\n",
    "    output_row[\"title\"] = title\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"translated_title\"] = translated_title\n",
    "    output_row[\"translated_utterances\"] = translated_utterances\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
