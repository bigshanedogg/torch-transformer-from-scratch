{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"Hodong_Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T02:55:10.811070Z",
     "start_time": "2021-06-15T02:55:09.470506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer.preprocessors.blender_bot_preprocessor import GeneratorFinetuningPreprocessor\n",
    "from transformer.data.dataset import DatasetInterface, DatasetFromDir\n",
    "from transformer.data.blender_bot_data_loader import GeneratorFinetuningDataLoader\n",
    "from transformer.models.transformer import Transformer\n",
    "from transformer.trainers.blender_bot_trainer import GeneratorFinetuningTransformerTrainer\n",
    "from transformer.trainers.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T02:55:10.856060Z",
     "start_time": "2021-06-15T02:55:10.812801Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-494df1a4b7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Picas_Server\n",
    "# dataset_dir = \"/home/picas/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# Korea_Server\n",
    "dataset_dir = \"/home/mnt/guest1\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)\n",
    "\n",
    "# # bigshane_local\n",
    "# dataset_dir = \"D:\\_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # AWS\n",
    "# dataset_dir = \"/home/ubuntu/data\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./scripts/transformer/config/generator_finetuning_korea.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_language = \"kor\"\n",
    "# tgt_language = \"kor\"\n",
    "# encoding = \"utf-8\"\n",
    "# src_vocab_size = tgt_vocab_size = 150000\n",
    "\n",
    "# src_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=src_language, vocab_size=config[\"model\"][\"src_vocab_size\"])\n",
    "# tgt_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=tgt_language, vocab_size=config[\"model\"][\"tgt_vocab_size\"])\n",
    "# trfr_prep = GeneratorFinetuningPreprocessor(src_language=src_language, tgt_language=tgt_language, src_spm_model_path=src_spm_model_path, tgt_spm_model_path=tgt_spm_model_path, embedding_dict=config[\"model\"][\"embedding_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported konlpy.tag.Mecab successfully\n",
      "loaded spm_model: '/Users/aibud_dev/_jupyter/spm_model/kor/spoken_pretrain_spm_v30000/'\n"
     ]
    }
   ],
   "source": [
    "src_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=config[\"data\"][\"src_language\"], vocab_size=config[\"model\"][\"src_vocab_size\"])\n",
    "tgt_spm_model_path = dataset_dir + \"/spm_model/{language}/spoken_pretrain_spm_v{vocab_size}\".format(language=config[\"data\"][\"tgt_language\"], vocab_size=config[\"model\"][\"tgt_vocab_size\"])\n",
    "# src_spm_model_path = config[\"data\"][\"src_spm_model_path\"].format(root_dir=config[\"data\"][\"root_dir\"], language=config[\"data\"][\"src_language\"], vocab_size=config[\"model\"][\"src_vocab_size\"])\n",
    "# tgt_spm_model_path = config[\"data\"][\"tgt_spm_model_path\"].format(root_dir=config[\"data\"][\"root_dir\"], language=config[\"data\"][\"tgt_language\"], vocab_size=config[\"model\"][\"tgt_vocab_size\"])\n",
    "preprocessor = GeneratorFinetuningPreprocessor(src_language=config[\"data\"][\"src_language\"], tgt_language=config[\"data\"][\"tgt_language\"], src_spm_model_path=src_spm_model_path, tgt_spm_model_path=tgt_spm_model_path, embedding_dict=config[\"model\"][\"embedding_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'temp_dir' has been set to '/Users/aibud_dev/_jupyter/model/temp/20210826_180449/' to save model while training\n",
      "LearningRate schedule has been set to 'transformer_lambda'\n"
     ]
    }
   ],
   "source": [
    "trainer = GeneratorFinetuningTransformerTrainer(temp_dir=dataset_dir+\"/model/temp/\")\n",
    "# trainer = GeneratorFinetuningTransformerTrainer(temp_dir=config[\"train\"][\"temp_save_path\"])\n",
    "# trainer.set_lr_update(initial_learning_rate=config[\"optimizer\"][\"initial_learning_rate\"], num_warmup_steps=config[\"train\"][\"num_warmup_steps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(src_pad_token_id=preprocessor.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], tgt_pad_token_id=preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], **config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = dataset_dir + \"/model/transformer/dialog_pretrain/20210821/epoch_100/\"\n",
    "transformer = load_state_dict(object=transformer, path=model_dir + ModelFilenameConstants.MODEL_STATE_DICT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set criterions & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions, criterion_weights = trainer.get_criterions(tgt_timesteps=config[\"model\"][\"tgt_timesteps\"], tgt_vocab_size=config[\"model\"][\"tgt_vocab_size\"], tgt_pad_token_id=preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"], **config[\"criterion\"])\n",
    "optimizer = trainer.get_optimizer(model=transformer, **config[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model device: cpu\n",
      "Setting criterions device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = GeneratorFinetuningTransformerTrainer.set_device(obj=transformer, device=device)\n",
    "optimizer = GeneratorFinetuningTransformerTrainer.set_device(obj=optimizer, device=device)\n",
    "criterions = GeneratorFinetuningTransformerTrainer.set_device(obj=criterions, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_params\n",
    "batch_size = 16\n",
    "nprocs = 1\n",
    "\n",
    "total_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v3/\"\n",
    "sample_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v3/sample/\"\n",
    "train_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v3/train/\"\n",
    "val_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/multi_turn_v3/val/\"\n",
    "\n",
    "train_dataset = DatasetFromDir(data_dir=train_data_dir, batch_size=batch_size, device=device, nprocs=nprocs, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"])\n",
    "train_data_loader_params = trainer.get_data_loader_params(dataset=train_dataset, preprocessor=preprocessor, batch_size=batch_size, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "train_data_loader = trainer.create_data_loader(**train_data_loader_params)\n",
    "\n",
    "val_dataset = DatasetFromDir(data_dir=val_data_dir, batch_size=batch_size, device=device, nprocs=nprocs, encoding=config[\"data\"][\"encoding\"], extension=config[\"data\"][\"extension\"])\n",
    "val_data_loader_params = trainer.get_data_loader_params(dataset=val_dataset, preprocessor=preprocessor, batch_size=batch_size, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "val_data_loader = trainer.create_data_loader(**val_data_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram = 5\n",
    "# utterances = [utterance for row in val_dataset.get_all_data() for utterance in row[\"utterances\"]]\n",
    "# target_prev_token_distribution, special_token_ids = preprocessor.extract_prev_token_distribution(sentences=utterances, ngram=ngram)\n",
    "# trainer.set_prev_token_distribution(prev_token_distribution=target_prev_token_distribution, special_token_ids=special_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting context_rows & candidate_rows: 100%|██████████| 51779/51779 [00:00<00:00, 141980.46it/s]\n",
      "Extracting length_list:   1%|          | 284/51779 [00:00<00:35, 1430.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context & condition sample: [['맞아요. 돌아오지 않는 시간들이니까요. ', '다시 돌아가고 싶어요.'], ['저도 돌아가고 싶다는 생각을 한 적 이있었는데 지금은 계속 과거를 아쉬워하는 것보단 지금 행복을 충분히 느끼면 좋겠다고 생각해요.']]\n",
      "candidate sample: ['저도 돌아가고 싶다는 생각을 한 적 이있었는데 지금은 계속 과거를 아쉬워하는 것보단 지금 행복을 충분히 느끼면 좋겠다고 생각해요.']\n",
      "context & condition "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting length_list: 100%|██████████| 51779/51779 [00:38<00:00, 1360.19it/s]\n",
      "Extracting length_list:   1%|          | 591/51779 [00:00<00:08, 5906.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1.000\tMax: 969.000\tAvg: 91.102\tQ1: 48.000\tQ2: 78.000\tQ3: 120.000\n",
      "candidate "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting length_list: 100%|██████████| 51779/51779 [00:08<00:00, 5860.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1.000\tMax: 201.000\tAvg: 21.516\tQ1: 12.000\tQ2: 18.000\tQ3: 27.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, 969, 48.0, 78.0, 120.0, 91.10185596477336),\n",
       " (1, 201, 12.0, 18.0, 27.0, 21.516135885204427))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader.summary(show_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader encode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_input_token:\t [0, 49, 59]\n",
      "src_input_segment:\t [0, 49, 59]\n",
      "src_input_token:\t <cls><spk1> 요즘 은 그런 분 들 많 잖아요 . 자신 의 얘기 를 연기 하 는 것 도 독보 적 인 능력 이 죠 .<spk2> 제 가 그럴 능력 까진 .<spk1> 일단 해 보 는 거 죠 . 사람 일 모르 잖 아요 ?<spk2> 할 이야기 가 많 긴 해요 .<spk1> 그것 들 이 다 자신 만 의 이야기 가 되 는 거 죠 .<spk2> 제 경험 들 ?<sep> test condition 입니다 .<sep>\n",
      "tgt_input_token:\t <spk1> 그렇 죠 . 누군가 는 재밌 어 하 지 않 을까요 ?\n",
      "tgt_output_token:\t 그렇 죠 . 누군가 는 재밌 어 하 지 않 을까요 ?\n",
      "\n",
      "src_input_token:\t <cls><spk2> 이렇게 얘기 를 하 다 보 니 제 가 뭘 원하 는지 조금 씩 보이 기 시작 하 네요 .<sep> test condition 입니다 .<sep>\n",
      "tgt_input_token:\t <spk1> 조금 이 라도 도움 이 되 셨 다니 다행 이 예요\n",
      "tgt_output_token:\t 조금 이 라도 도움 이 되 셨 다니 다행 이 예요\n",
      "\n",
      "src_input_token:\t <cls><spk1> 오 징어 , 삼겹살 , 치킨 정말 맛있 죠<spk2> 맞 아요 정말 맛있 어요<spk1> 오징어 를 좋아하 는 이유 있 으세요 ?<spk2> 어 일단 쫄깃 하 고 씹 는 식감 이 좋 더라구요<sep> test condition 입니다 .<sep>\n",
      "tgt_input_token:\t <spk1> 맞 아요 오징어 는 그런 점 이 좋 더라구요 . 삼겹살 은 좋 아 하 시 는 이유 가 있 으세요 ?\n",
      "tgt_output_token:\t 맞 아요 오징어 는 그런 점 이 좋 더라구요 . 삼겹살 은 좋 아 하 시 는 이유 가 있 으세요 ?\n",
      "\n",
      "src_input_token:\t <cls><spk2> 학교 도서관 리모 델 링 하 고 , 식당 도 바뀌 고 그렇 다는데 잘 이용 을 못 했 어요 .<spk1> 학교 를 잘 못 가 니깐 그렇 겠 네요 . 코로나 때문 에 밥 먹 는 것 도 쉽 지 않 을 테 고요 .<spk2> 이번 에 기말고사 를 치러 갔 을 때 겨우 시간 이 맞 아서 도서관 과서 이용 을 해 봤 어요 . 밥 먹 을 시간 은 안 돼서 식당 은 구경 도 못했 지만 요 .<sep> test condition 입니다 .<sep>\n",
      "tgt_input_token:\t <spk1> 그래도 하나 라도 해 볼 수 있 어서 다행 이 네요 . 시험 기간 이 었 어서 밥 먹 는 시간 을 내 기 어렵 죠 .\n",
      "tgt_output_token:\t 그래도 하나 라도 해 볼 수 있 어서 다행 이 네요 . 시험 기간 이 었 어서 밥 먹 는 시간 을 내 기 어렵 죠 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_idx = 2\n",
    "target_idx = 0\n",
    "for batch_idx, batch in enumerate(train_data_loader):\n",
    "    src_inputs, tgt_inputs, tgt_outputs = batch\n",
    "    if batch_idx >= target_idx: break\n",
    "\n",
    "print(\"src_input_token:\\t\", [token_idx for token_idx in range(0, len(src_inputs[\"token\"][row_idx])) if token_idx==0 or src_inputs[\"token\"][row_idx][token_idx]==preprocessor.src_spm_tokenizer.special_token_dict[\"sep\"][\"id\"]])\n",
    "print(\"src_input_segment:\\t\", [token_idx for token_idx in range(0, len(src_inputs[\"segment\"][row_idx])-1) if token_idx==0 or src_inputs[\"segment\"][row_idx][token_idx]!=src_inputs[\"segment\"][row_idx][token_idx+1]])\n",
    "# print(\"src_input_turn:\\t\", [token_idx for token_idx in range(0, len(src_inputs[\"turn\"][row_idx])-1) if token_idx==0 or src_inputs[\"turn\"][row_idx][token_idx]!=src_inputs[\"turn\"][row_idx][token_idx+1]])\n",
    "\n",
    "for src_input_token, tgt_input_token, tgt_output_token in zip(preprocessor.src_decode(src_inputs[\"token\"]), preprocessor.tgt_decode(tgt_inputs[\"token\"]), preprocessor.tgt_decode(tgt_outputs[\"lm\"])):\n",
    "    print(\"src_input_token:\\t\", src_input_token)\n",
    "    print(\"tgt_input_token:\\t\", tgt_input_token)\n",
    "    print(\"tgt_output_token:\\t\", tgt_output_token)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibud_dev/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "amp = True\n",
    "scaler = None\n",
    "if amp: scaler = torch.cuda.amp.GradScaler()\n",
    "save_per_epoch = -1\n",
    "save_per_batch = -1\n",
    "keep_last = True\n",
    "verbose_per_epoch = 1\n",
    "verbose_per_batch = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.fit(model=transformer, train_data_loader=train_data_loader, val_data_loader=val_data_loader, \n",
    "                      criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                      epoch=epoch, amp=amp, save_per_epoch=save_per_epoch, save_per_batch=save_per_batch, keep_last=keep_last, verbose_per_epoch=verbose_per_epoch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = tqdm(train_data_loader, initial=train_data_loader.iter_start, total=len(train_data_loader))\n",
    "data_iter.iter_size = train_data_loader.iter_end - train_data_loader.iter_start\n",
    "epoch_train_history = trainer.train_epoch(model=transformer, data_loader=data_iter, \n",
    "                                          criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, device=device, \n",
    "                                          amp=amp, scaler=scaler, save_per_batch=save_per_batch, verbose_per_batch=verbose_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-db153710a2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mbatch_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_data_loader):\n",
    "    batch_idx += 1\n",
    "    batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "    \n",
    "    loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "                                            criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                            train=True, amp=amp, scaler=scaler)\n",
    "    \n",
    "    print(loss_dict)\n",
    "    print(acc_dict)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.iteration & data_loader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch = [next(train_data_loader.dataset.__iter__()) for i in range(0, batch_size)]\n",
    "batch_idx = 1\n",
    "batch = train_data_loader.collate_fn(batch=_batch)\n",
    "batch = [{k: trainer.convert_to_tensor(data=v, device=device) for k, v in _batch.items()} for _batch in batch]\n",
    "\n",
    "loss_dict, acc_dict = trainer.iteration(model=transformer, batch=batch,\n",
    "                                        criterions=criterions, criterion_weights=criterion_weights, optimizer=optimizer, \n",
    "                                        train=True, amp=amp, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_params = trainer.get_data_loader_params(dataset=train_dataset, preprocessor=preprocessor, batch_size=1, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "train_data_loader = trainer.create_data_loader(**train_data_loader_params)\n",
    "val_data_loader_params = trainer.get_data_loader_params(dataset=val_dataset, preprocessor=preprocessor, batch_size=1, device=device, nprocs=nprocs, **config[\"model\"], **config[\"data_loader\"])\n",
    "val_data_loader = trainer.create_data_loader(**val_data_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def loader_iter(data_loader):\n",
    "    for batch in data_loader:\n",
    "        yield batch\n",
    "\n",
    "train_gen = loader_iter(train_data_loader)\n",
    "val_gen = loader_iter(val_data_loader)\n",
    "\n",
    "transformer.eval()\n",
    "src_pad_token_id = preprocessor.src_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]\n",
    "tgt_pad_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"pad\"][\"id\"]\n",
    "tgt_bos_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"speaker_1\"][\"id\"]\n",
    "tgt_eos_token_id = preprocessor.tgt_spm_tokenizer.special_token_dict[\"eos\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = train_gen\n",
    "src_inputs = {\"token\":[]}\n",
    "while len(src_inputs[\"token\"]) < 1:\n",
    "    batch = next(gen)\n",
    "    src_inputs, tgt_inputs, tgt_outputs = batch\n",
    "\n",
    "context = preprocessor.src_decode(src_inputs[\"token\"])[0]\n",
    "greedy_prediction = transformer.inference_greedy(src_inputs=src_inputs, src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, tgt_bos_token_id=tgt_bos_token_id, tgt_eos_token_id=tgt_eos_token_id)\n",
    "greedy_reply = preprocessor.tgt_decode(greedy_prediction)[0]\n",
    "beam_prediction, probs = transformer.inference_beam_search(src_inputs=src_inputs, src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, tgt_bos_token_id=tgt_bos_token_id, tgt_eos_token_id=tgt_eos_token_id)\n",
    "beam_replies = preprocessor.tgt_decode(beam_prediction)\n",
    "sampling_prediction, probs = transformer.inference_random_sampling(src_inputs=src_inputs, src_pad_token_id=src_pad_token_id, tgt_pad_token_id=tgt_pad_token_id, tgt_bos_token_id=tgt_bos_token_id, tgt_eos_token_id=tgt_eos_token_id, num_samples=5, temperature=0.7)\n",
    "sampling_replies = preprocessor.tgt_decode(sampling_prediction)\n",
    "\n",
    "ctxt_list = re.split(\"(<spk1>|<spk2>)\", context)[1:]\n",
    "for i in range(0, len(ctxt_list), 2):\n",
    "    print(\"{}: {}\".format(ctxt_list[i], ctxt_list[i+1]))\n",
    "print(\"{}: {}\".format(\"<spk1>(greedy)\", greedy_reply))\n",
    "for beam_reply in beam_replies:\n",
    "    print(\"{}: {}\".format(\"<spk1>(beam)\", beam_reply))\n",
    "print(\"{}: {}\".format(\"<spk1>(sampling)\", sampling_replies[0]))\n",
    "print(\"({}: {})\".format(\"ans\", preprocessor.tgt_decode(tgt_outputs[\"lm\"])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_dir = trainer.temp_dir + \"epoch_13/\"\n",
    "# trainer.save(path=_model_dir, model=transformer, optimizer=optimizer, history=None, config=config, preprocessor=preprocessor, save_model_hyperparams=True, save_optimizer_hyperparams=False, ddp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.services.dialog_generator.transformer import DialogGenerator\n",
    "dg = DialogGenerator(temp_dir=\"./\")\n",
    "dg.load_model(model_dir=_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [\n",
    "    \"여기 있는 사람들이 다 롤러코스터 타려고 기다리는 사람들이야?\",\n",
    "    \"그런 것 같아요.\",\n",
    "    \"얼마나 기다려야 할까?\",\n",
    "    \"최소한 한 시간 반 정도 기다려야 될 것 같아요.\",\n",
    "    \"한 시간 반?\",\n",
    "    \"줄이 너무 기니까 우리 다른 것부터 탈까?\"\n",
    "]\n",
    "speaker_ids = [1, 0, 1, 0, 1, 1]\n",
    "conditions = None # [\"condition 문장입니다.\"]\n",
    "beam_size = 5\n",
    "min_length = 5\n",
    "lp_alpha = 1.2\n",
    "lp_min_length = 5\n",
    "return_probs = False\n",
    "max_retry = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy\n",
    "dg.infer_next_utterance_greedy(utterances=utterances, speaker_ids=speaker_ids, conditions=conditions, max_retry=max_retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search\n",
    "dg.infer_next_utterance_beam_search(utterances=utterances, speaker_ids=speaker_ids, conditions=conditions,\n",
    "                                    beam_size=beam_size, min_length=min_length, lp_alpha=lp_alpha, lp_min_length=lp_min_length, return_probs=return_probs,\n",
    "                                    max_retry=max_retry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
